# AlgVex æœªæ¥è§„åˆ’ (P3 - è·¯çº¿å›¾ä¸å†å²)

> **Qlib + Hummingbot èåˆçš„ä¸“ä¸šåŠ å¯†è´§å¸é‡åŒ–äº¤æ˜“å¹³å°**
>
> æœ¬æ–‡æ¡£åŒ…å«å¼€å‘è·¯çº¿å›¾ã€æ›´æ–°æ—¥å¿—å’Œæ–‡æ¡£æ€»ç»“ã€‚
>
> ç›¸å…³æ–‡æ¡£ï¼š
> - [æ ¸å¿ƒåŠŸèƒ½ (P0)](./CORE_PLAN.md) - MVP å¿…é¡»å®ç°
> - [æ‰©å±•åŠŸèƒ½ (P2)](./EXTENSION_PLAN.md) - å› å­æ‰©å±•ã€é£æ§å¢å¼ºç­‰

---

## ç›®å½•

- [æ›´æ–°æ—¥å¿—](#ğŸ“‹-v200-æ›´æ–°æ—¥å¿—-2025-12-31)
- [12. å¼€å‘è·¯çº¿å›¾](#12-å¼€å‘è·¯çº¿å›¾)
- [æ–‡æ¡£æ€»ç»“](#æ–‡æ¡£æ€»ç»“)

---

## ğŸ“‹ v2.0.0 æ›´æ–°æ—¥å¿— (2025-12-31)

### ğŸ†• é‡å¤§æ›´æ–°: Qlib + Hummingbot å®Œæ•´åŠŸèƒ½å®ç°

#### 1. Qlib æ¨¡å‹å°è£…å±‚ (`research/qlib_models.py`)

å®Œæ•´å°è£… Qlib 0.9.7 æ‰€æœ‰ 25+ æ¨¡å‹:

| æ¨¡å‹ç±»åˆ« | æ¨¡å‹åˆ—è¡¨ |
|----------|----------|
| **GBDT** | LightGBM, XGBoost, CatBoost |
| **çº¿æ€§æ¨¡å‹** | Linear, Ridge, Lasso |
| **åŸºç¡€DL** | LSTM, GRU, MLP, TCN |
| **é«˜çº§DL** | Transformer, ALSTM, TabNet, GATS, SFM, HIST, TRA |
| **é›†æˆæ¨¡å‹** | DoubleEnsemble |
| **å…¶ä»–** | GAT, IGMTF, ADD, ADARNN, TCTS, Localformer |

```python
from algvex.research.qlib_models import ModelFactory, ModelType

# åˆ›å»ºæ¨¡å‹
model = ModelFactory.create(ModelType.TRANSFORMER, d_model=64, n_heads=8)
model.fit(dataset)
predictions = model.predict(dataset)
```

#### 2. äº¤æ˜“æ‰€è¿æ¥å™¨ (`core/execution/exchange_connectors.py`)

æ”¯æŒå¤šäº¤æ˜“æ‰€æ°¸ç»­åˆçº¦äº¤æ˜“:

| äº¤æ˜“æ‰€ | åŠŸèƒ½ |
|--------|------|
| **Binance Perpetual** | è®¢å•ã€æŒä»“ã€è´¦æˆ·ã€Kçº¿ã€èµ„é‡‘è´¹ç‡ |
| **Bybit Perpetual** | è®¢å•ã€æŒä»“ã€è´¦æˆ·ã€Kçº¿ã€èµ„é‡‘è´¹ç‡ |
| **OKX** (é¢„ç•™) | æ¶æ„å·²æ”¯æŒ |
| **Gate.io** (é¢„ç•™) | æ¶æ„å·²æ”¯æŒ |

```python
from algvex.core.execution.exchange_connectors import (
    BinancePerpetualConnector, BybitPerpetualConnector,
    ExchangeConfig, ExchangeType, OrderRequest, OrderSide, OrderType
)
from decimal import Decimal

# åˆ›å»ºé…ç½®
config = ExchangeConfig(
    exchange_type=ExchangeType.BINANCE_PERPETUAL,
    api_key="your_api_key",
    api_secret="your_api_secret",
    testnet=True  # ä½¿ç”¨æµ‹è¯•ç½‘
)

# åˆ›å»ºè¿æ¥å™¨
connector = BinancePerpetualConnector(config)
await connector.connect()

# åˆ›å»ºè®¢å•
order_request = OrderRequest(
    symbol="BTCUSDT",
    side=OrderSide.BUY,
    order_type=OrderType.MARKET,
    quantity=Decimal("0.01")
)
order = await connector.create_order(order_request)

# æŸ¥è¯¢æŒä»“
positions = await connector.get_positions()
```

#### 3. æ‰§è¡Œç­–ç•¥ (`core/execution/executors.py`)

å®ç° 5 ç§ä¸“ä¸šæ‰§è¡Œç®—æ³•:

| ç­–ç•¥ | è¯´æ˜ | ç”¨é€” |
|------|------|------|
| **TWAP** | æ—¶é—´åŠ æƒå¹³å‡ä»·æ ¼ | å¤§å•æ‹†åˆ†ï¼Œå‡å°‘å†²å‡» |
| **VWAP** | æˆäº¤é‡åŠ æƒå¹³å‡ä»·æ ¼ | è·Ÿè¸ªå¸‚åœºæˆäº¤åˆ†å¸ƒ |
| **Grid** | ç½‘æ ¼äº¤æ˜“ | éœ‡è¡è¡Œæƒ…ç›ˆåˆ© |
| **DCA** | å®šæŠ•ç­–ç•¥ | åˆ†æ‰¹å»ºä»“ |
| **Iceberg** | å†°å±±è®¢å• | éšè—å¤§å•æ„å›¾ |

```python
from algvex.core.execution.executors import TWAPExecutor, GridExecutor
from algvex.core.execution.exchange_connectors import OrderSide
from decimal import Decimal

# TWAP æ‰§è¡Œ
executor = TWAPExecutor(
    connector=connector,
    symbol="BTCUSDT",
    side=OrderSide.BUY,
    total_quantity=Decimal("0.1"),
    duration_minutes=60,
    num_slices=12
)
result = await executor.execute()

# ç½‘æ ¼äº¤æ˜“
executor = GridExecutor(
    connector=connector,
    symbol="BTCUSDT",
    total_quantity=Decimal("1.0"),
    lower_price=Decimal("40000"),
    upper_price=Decimal("45000"),
    num_grids=10
)
result = await executor.execute()
```

#### 4. HummingbotBridge v2.0.0 é‡å†™

å®Œå…¨é‡å†™çš„æ‰§è¡Œæ¡¥æ¥å±‚:

- å¤šäº¤æ˜“æ‰€æ”¯æŒ
- å¤šæ‰§è¡Œç­–ç•¥æ”¯æŒ
- å¼‚æ­¥è®¢å•ç®¡ç†
- è‡ªåŠ¨é‡è¿æœºåˆ¶
- å®Œæ•´çš„çŠ¶æ€åŒæ­¥

---

## ğŸ“‹ v5.1.0 æ›´æ–°æ—¥å¿— (2025-12-23)

### ğŸ†• æ–°å¢åŠŸèƒ½

#### 1. è·¨æˆªé¢å¤„ç†å™¨ (Qlib åŸç‰ˆé€‚é…)

| å¤„ç†å™¨ | è¯´æ˜ | ç”¨æ³• |
|--------|------|------|
| `CSZScoreNorm` | è·¨æˆªé¢ Z-Score æ ‡å‡†åŒ– | æ¯ä¸ªæ—¶é—´ç‚¹ç‹¬ç«‹è®¡ç®— z-score |
| `CSRankNorm` | è·¨æˆªé¢æ’åæ ‡å‡†åŒ– | å…¬å¼: (rank(pct=True) - 0.5) * 3.46 |
| `CSFillna` | è·¨æˆªé¢ç¼ºå¤±å€¼å¡«å…… | ç”¨åŒä¸€æ—¶é—´ç‚¹çš„å‡å€¼å¡«å…… |
| `TanhProcess` | Tanh å»å™ªå¤„ç† | å‹ç¼©æç«¯å€¼ |
| `ProcessInf` | æ— ç©·å€¼å¤„ç† | æ›¿æ¢ inf/-inf |
| `FilterCol` | åˆ—è¿‡æ»¤å™¨ | ä¿ç•™æŒ‡å®šåˆ— |
| `DropCol` | åˆ—åˆ é™¤å™¨ | åˆ é™¤æŒ‡å®šåˆ— |

```python
from algvex.core.factor import CSZScoreNorm, CSRankNorm, TanhProcess

# ä½¿ç”¨ç¤ºä¾‹
processors = ProcessorChain([
    CSZScoreNorm(),      # è·¨æˆªé¢ z-score
    CSRankNorm(),        # è·¨æˆªé¢æ’å
    TanhProcess(),       # tanh å»å™ª
])
```

#### 2. è¯„ä¼°æ¨¡å— (Qlib åŸç‰ˆ)

| å‡½æ•° | è¯´æ˜ |
|------|------|
| `risk_analysis(returns)` | å¹´åŒ–æ”¶ç›Šã€å¤æ™®æ¯”ç‡ã€æœ€å¤§å›æ’¤ |
| `calc_ic(pred, label)` | IC å’Œ Rank IC è®¡ç®— |
| `calc_long_short_return()` | å¤šç©ºæ”¶ç›Šåˆ†æ |
| `calc_long_short_prec()` | å¤šç©ºç²¾åº¦åˆ†æ |
| `generate_report()` | ç»¼åˆè¯„ä¼°æŠ¥å‘Š |

```python
from algvex.core import risk_analysis, calc_ic, generate_report

# é£é™©åˆ†æ
metrics = risk_analysis(returns, freq='day')
print(f"å¤æ™®æ¯”ç‡: {metrics['information_ratio']:.2f}")

# IC åˆ†æ
ic, rank_ic = calc_ic(predictions, labels)

# ç»¼åˆæŠ¥å‘Š
report = generate_report(predictions, labels, returns)
```

#### 3. Qlib é£æ ¼æ¨¡å‹æ¥å£

| æ¨¡å‹ | è¯´æ˜ |
|------|------|
| `LGBModel` | LightGBMï¼Œæ”¯æŒ fit/predict/finetune |
| `XGBModel` | XGBoostï¼Œå¸¦ç‰¹å¾é‡è¦æ€§ |
| `LinearModel` | OLS, NNLS, Ridge, Lasso |
| `get_model()` | ä¾¿æ·å·¥å‚å‡½æ•° |

```python
from algvex.core.model import LGBModel, LinearModel, get_model

# LightGBM æ¨¡å‹
model = LGBModel(num_leaves=64, learning_rate=0.05)
model.fit(dataset)
predictions = model.predict(dataset, segment='test')

# çº¿æ€§æ¨¡å‹
model = LinearModel(estimator='ridge', alpha=0.1)
model.fit(dataset)

# å¾®è°ƒ
model.finetune(new_dataset, num_boost_round=10)
```

---

## 12. å¼€å‘è·¯çº¿å›¾

> ğŸ“‹ **å‚è€ƒæ–‡æ¡£** - æ­¤è·¯çº¿å›¾ä¸ºå…¨æ™¯è§„åˆ’ï¼Œ**MVPä»…éœ€å®ŒæˆPhase 0 + Phase 1æ ¸å¿ƒéƒ¨åˆ†**ã€‚
> â¸ï¸ **MVPä¸åŒ…å«** - Phase 2/3çš„180å› å­æ‰©å±•ã€é“¾ä¸Šæ•°æ®ã€ç¤¾åª’æ–°é—»ç­‰å»¶åå®ç°ã€‚

> **åŸåˆ™**: å…ˆè®©ç³»ç»Ÿ"å¯ä¿¡"ï¼ˆæ•°æ®å¯å¤ç°ï¼‰ï¼Œå†åš"å¯ç”¨"ï¼ˆæœ€å°å¯äº¤æ˜“ï¼‰ï¼Œæœ€ååš"ä¸°å¯Œ"ï¼ˆ180å› å­ï¼‰ã€‚

### 12.1 é˜¶æ®µæ€»è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          å¼€å‘é˜¶æ®µä¸ä¾èµ–å…³ç³»                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Phase 0: æ•°æ®åŸºç¡€è®¾æ–½ (è®©ç³»ç»Ÿ"å¯ä¿¡")                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Step 1: æ•°æ®é‡‡é›†å™¨å®ç°                                               â”‚   â”‚
â”‚  â”‚ Step 2: B/Cæ¡£æ•°æ®è½ç›˜                                                â”‚   â”‚
â”‚  â”‚ Step 3: æ•°æ®è¡€ç¼˜ä¸å¿«ç…§                                               â”‚   â”‚
â”‚  â”‚ Step 7: æ•°æ®è´¨é‡ç›‘æ§                                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“                                              â”‚
â”‚  Phase 1: å›æµ‹å¯ä¿¡æ€§ + P1æ•°æ®æ‰©å±• (è®©å›æµ‹"å¯ä¿¡")                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Step 4: å›æµ‹-å®ç›˜æˆäº¤å¯¹é½ (DynamicSlippageModel)                     â”‚   â”‚
â”‚  â”‚ Step 5: Walk-ForwardéªŒè¯æµç¨‹                                         â”‚   â”‚
â”‚  â”‚ â˜… Step 9: L2æ·±åº¦èšåˆ + æ»‘ç‚¹æ ¡å‡† (CalibratedSlippageModel)            â”‚   â”‚
â”‚  â”‚ â˜… Step 10: æ¸…ç®—æ•°æ® (LiquidationCollector + çº§è”æ£€æµ‹)                â”‚   â”‚
â”‚  â”‚ â˜… Step 11: å¤šäº¤æ˜“æ‰€Basis (Binance/Bybit/OKX ä»·å·®çŸ©é˜µ)                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“                                              â”‚
â”‚  Phase 2: P0éªŒæ”¶ä¸CI/CD                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Step 6: P0å•å…ƒæµ‹è¯•                                                   â”‚   â”‚
â”‚  â”‚ Step 8: CI/CDé›†æˆ                                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“                                              â”‚
â”‚  Phase 3: æœ€å°å¯äº¤æ˜“ç³»ç»Ÿ                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ä½¿ç”¨30-60ä¸ªç¨³å®šå› å­è®­ç»ƒbaselineæ¨¡å‹                                  â”‚   â”‚
â”‚  â”‚ å®ç›˜å½±å­æ¨¡å¼ (paper trading) éªŒè¯                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„ åç»­æ‰©å±• (è§12.14) â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„        â”‚
â”‚  P2: é“¾ä¸Šæµå‘äº¤æ˜“æ‰€ | P2: æ›´ç»†IVç»“æ„ | P3: ç¤¾åª’/æ–°é—»                        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Step 9 ä¸ºä½•ä¼˜å…ˆ**:
1. **ç›´æ¥æå‡ P0-6 å¯ä¿¡åº¦** - å½“å‰ DynamicSlippageModel åŸºäºç»éªŒå…¬å¼ä¼°ç®—ï¼Œæœ‰çœŸå®æ·±åº¦æ•°æ®å¯éªŒè¯/æ ¡å‡†
2. **å·¥ç¨‹å¤æ‚åº¦å¯æ§** - åªéœ€ bar èšåˆç‰ˆ (1m/5m)ï¼Œä¸éœ€è¦æ¯«ç§’çº§
3. **å›æµ‹/å®ç›˜åŒå‘å—ç›Š** - æ—¢èƒ½æ”¹è¿›å›æµ‹æ»‘ç‚¹æ¨¡å‹ï¼Œä¹Ÿèƒ½ç”¨äºå®ç›˜ä¸‹å•å‰é¢„ä¼°

### 12.2 Step 1: æ•°æ®é‡‡é›†å™¨å®ç°

**ç›®æ ‡**: å®ç°æ‰€æœ‰ Collector ç±»ï¼Œç¡®ä¿ fetch_historical å’Œ subscribe_realtime æ–¹æ³•ç¬¦åˆ DataManager è§„èŒƒã€‚

**æ–‡ä»¶ä½ç½®**: `algvex/core/data/collectors/`

```python
# éœ€è¦å®ç°çš„é‡‡é›†å™¨
algvex/core/data/collectors/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ base.py              # BaseCollector æŠ½è±¡ç±»
â”œâ”€â”€ binance.py           # BinanceCollector (OHLCV, OI, LS, Taker, Funding)
â”œâ”€â”€ deribit.py           # DeribitCollector (DVOL, IV, Put/Call, MaxPain)
â”œâ”€â”€ defilama.py          # DefiLlamaCollector (TVL, Stablecoin)
â”œâ”€â”€ sentiment.py         # SentimentCollector (Fear&Greed, Google Trends)
â””â”€â”€ macro.py             # MacroCollector (DXY, Yields, SPX, VIX)
```

**å…³é”®å®ç°è¦ç‚¹**:

```python
class BinanceCollector(BaseCollector):
    """å¸å®‰æ•°æ®é‡‡é›†å™¨"""

    # 1. APIé™æµé…ç½® (å¿…é¡»éµå®ˆï¼Œå¦åˆ™ä¼šè¢«å°IP)
    RATE_LIMITS = {
        "klines": {"weight": 1, "limit": 1200, "window": 60},  # 1200/åˆ†é’Ÿ
        "openInterest": {"weight": 1, "limit": 1200, "window": 60},
        "topLongShortRatio": {"weight": 1, "limit": 1200, "window": 60},
    }

    # 2. é‡è¯•é…ç½®
    RETRY_CONFIG = {
        "max_retries": 3,
        "backoff_factor": 2,  # 2s, 4s, 8s
        "retry_on": [429, 500, 502, 503, 504],
    }

    # 3. é”™è¯¯å¤„ç†
    async def fetch_historical(self, symbol: str, start: str, end: str) -> pd.DataFrame:
        """è·å–å†å²æ•°æ® - å¸¦é™æµå’Œé‡è¯•"""
        try:
            await self._check_rate_limit("klines")
            data = await self._fetch_with_retry(...)
            return self._validate_and_clean(data)
        except RateLimitExceeded:
            await asyncio.sleep(self._get_backoff_time())
            return await self.fetch_historical(symbol, start, end)
        except Exception as e:
            self.logger.error(f"Fetch failed: {e}")
            raise DataFetchError(f"Failed to fetch {symbol}: {e}")

    # 4. æ•°æ®éªŒè¯
    def _validate_and_clean(self, df: pd.DataFrame) -> pd.DataFrame:
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing_ratio = df.isnull().sum() / len(df)
        if missing_ratio.max() > 0.1:  # è¶…è¿‡10%ç¼ºå¤±
            self.logger.warning(f"High missing ratio: {missing_ratio.max():.2%}")

        # æ£€æŸ¥å¼‚å¸¸å€¼
        # æ£€æŸ¥æ—¶é—´è¿ç»­æ€§
        # ...
        return df
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰ 5 ä¸ª Collector å®ç°å®Œæˆ
- [ ] æ¯ä¸ª Collector æœ‰å¯¹åº”çš„å•å…ƒæµ‹è¯•
- [ ] API é™æµé€»è¾‘é€šè¿‡å‹åŠ›æµ‹è¯•
- [ ] é”™è¯¯é‡è¯•é€»è¾‘è¦†ç›–å¸¸è§å¼‚å¸¸

---

### 12.3 Step 2: B/Cæ¡£æ•°æ®è½ç›˜

**ç›®æ ‡**: å°† B/C æ¡£æ•°æ®æºçš„æ•°æ®å®šæœŸæ‹‰å–å¹¶å­˜å…¥ TimescaleDBï¼Œå½¢æˆé•¿æœŸå†å²ã€‚

**è°ƒåº¦æ–¹æ¡ˆ**: ä½¿ç”¨ Celery Beat å®šæ—¶ä»»åŠ¡

```python
# algvex/tasks/data_collection.py

from celery import Celery
from celery.schedules import crontab

app = Celery('algvex')

# å®šæ—¶ä»»åŠ¡é…ç½®
app.conf.beat_schedule = {
    # Bæ¡£æ•°æ®: æ¯5åˆ†é’Ÿé‡‡é›†ä¸€æ¬¡
    'collect-oi-every-5min': {
        'task': 'tasks.collect_open_interest',
        'schedule': crontab(minute='*/5'),
        'args': (['BTCUSDT', 'ETHUSDT'],),
    },
    'collect-ls-ratio-every-5min': {
        'task': 'tasks.collect_long_short_ratio',
        'schedule': crontab(minute='*/5'),
        'args': (['BTCUSDT', 'ETHUSDT'],),
    },
    'collect-taker-volume-every-1min': {
        'task': 'tasks.collect_taker_volume',
        'schedule': crontab(minute='*/1'),
        'args': (['BTCUSDT', 'ETHUSDT'],),
    },

    # Bæ¡£æ•°æ®: æ¯å°æ—¶é‡‡é›†
    'collect-deribit-every-hour': {
        'task': 'tasks.collect_deribit_options',
        'schedule': crontab(minute=5),  # æ¯å°æ—¶ç¬¬5åˆ†é’Ÿ
        'args': (['BTC', 'ETH'],),
    },

    # Cæ¡£æ•°æ®: æ¯æ—¥é‡‡é›†
    'collect-google-trends-daily': {
        'task': 'tasks.collect_google_trends',
        'schedule': crontab(hour=1, minute=0),  # æ¯å¤©å‡Œæ™¨1ç‚¹
        'args': (['bitcoin', 'crypto'],),
    },

    # æ•°æ®è´¨é‡æ£€æŸ¥: æ¯å°æ—¶
    'check-data-quality-hourly': {
        'task': 'tasks.check_data_quality',
        'schedule': crontab(minute=30),
    },
}

@app.task(bind=True, max_retries=3)
def collect_open_interest(self, symbols: List[str]):
    """é‡‡é›†æŒä»“é‡æ•°æ®"""
    try:
        collector = BinanceCollector()
        for symbol in symbols:
            data = collector.fetch_open_interest(symbol)
            storage.save_to_timescale(data, table='binance_oi')
            logger.info(f"Collected OI for {symbol}: {len(data)} rows")
    except Exception as e:
        logger.error(f"OI collection failed: {e}")
        self.retry(exc=e, countdown=60)  # 1åˆ†é’Ÿåé‡è¯•
```

**TimescaleDB è¡¨ç»“æ„**:

```sql
-- Bæ¡£æ•°æ®è¡¨ (éœ€è¦é•¿æœŸç§¯ç´¯çš„æ•°æ®)
CREATE TABLE binance_oi (
    time        TIMESTAMPTZ NOT NULL,
    symbol      TEXT NOT NULL,
    open_interest DOUBLE PRECISION,
    open_interest_value DOUBLE PRECISION,
    collected_at TIMESTAMPTZ DEFAULT NOW()
);
SELECT create_hypertable('binance_oi', 'time');

CREATE TABLE binance_ls_ratio (
    time        TIMESTAMPTZ NOT NULL,
    symbol      TEXT NOT NULL,
    long_short_ratio DOUBLE PRECISION,
    long_account DOUBLE PRECISION,
    short_account DOUBLE PRECISION,
    collected_at TIMESTAMPTZ DEFAULT NOW()
);
SELECT create_hypertable('binance_ls_ratio', 'time');

-- æ•°æ®è½ç›˜å…ƒæ•°æ®è¡¨ (è¿½è¸ªé‡‡é›†çŠ¶æ€)
CREATE TABLE data_collection_log (
    id          SERIAL PRIMARY KEY,
    source      TEXT NOT NULL,
    symbol      TEXT NOT NULL,
    start_time  TIMESTAMPTZ NOT NULL,
    end_time    TIMESTAMPTZ NOT NULL,
    rows_collected INTEGER,
    status      TEXT,  -- 'success', 'partial', 'failed'
    error_message TEXT,
    created_at  TIMESTAMPTZ DEFAULT NOW()
);
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] Celery Beat å®šæ—¶ä»»åŠ¡é…ç½®å®Œæˆ
- [ ] TimescaleDB è¡¨ç»“æ„åˆ›å»ºå®Œæˆ
- [ ] Bæ¡£æ•°æ® (OI/LS/Taker) æ¯5åˆ†é’Ÿè‡ªåŠ¨é‡‡é›†
- [ ] Cæ¡£æ•°æ® (Google Trends) æ¯æ—¥è‡ªåŠ¨é‡‡é›†
- [ ] é‡‡é›†æ—¥å¿—å¯è¿½æº¯

---

### 12.4 Step 3: æ•°æ®è¡€ç¼˜ä¸å¿«ç…§

**ç›®æ ‡**: å®ç° DataSnapshot å’Œ ExperimentRecordï¼Œç¡®ä¿æ¯æ¬¡è®­ç»ƒ/å›æµ‹å¯å¤ç°ã€‚

**æ–‡ä»¶ä½ç½®**: `algvex/core/data/lineage.py`

**å¿«ç…§å­˜å‚¨æ–¹æ¡ˆ**:
- å¿«ç…§å…ƒæ•°æ® â†’ PostgreSQL
- å¿«ç…§æ•°æ®æ–‡ä»¶ â†’ æœ¬åœ° Parquet (æœªæ¥å¯è¿ç§»åˆ° S3)

```python
# algvex/core/data/lineage.py

import hashlib
from pathlib import Path

class DataLineageManager:
    """æ•°æ®è¡€ç¼˜ç®¡ç†å™¨"""

    SNAPSHOT_DIR = Path("~/.algvex/snapshots").expanduser()

    def __init__(self, db_url: str):
        self.db = Database(db_url)
        self.SNAPSHOT_DIR.mkdir(parents=True, exist_ok=True)

    def create_snapshot(self,
                       symbols: List[str],
                       start_date: str,
                       end_date: str,
                       data_manager: DataManager) -> str:
        """
        åˆ›å»ºæ•°æ®å¿«ç…§

        1. ä» DataManager è·å–æ•°æ®
        2. è®¡ç®—æ•°æ®å†…å®¹ hash
        3. ä¿å­˜åˆ° Parquet æ–‡ä»¶
        4. è®°å½•å…ƒæ•°æ®åˆ°æ•°æ®åº“
        """
        # 1. è·å–æ•°æ®
        df = data_manager.get_historical(
            symbols=symbols,
            start_date=start_date,
            end_date=end_date,
            fields="all",
        )

        # 2. ç”Ÿæˆå¿«ç…§ID (åŸºäºå†…å®¹hash)
        content_hash = hashlib.sha256(
            pd.util.hash_pandas_object(df).values.tobytes()
        ).hexdigest()[:16]
        snapshot_id = f"snap_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{content_hash}"

        # 3. ä¿å­˜æ•°æ®æ–‡ä»¶
        snapshot_path = self.SNAPSHOT_DIR / f"{snapshot_id}.parquet"
        df.to_parquet(snapshot_path, compression='zstd')

        # 4. è®°å½•å…ƒæ•°æ®
        snapshot = DataSnapshot(
            snapshot_id=snapshot_id,
            created_at=datetime.now(timezone.utc),
            symbols=symbols,
            start_date=start_date,
            end_date=end_date,
            source_versions=data_manager.get_source_versions(),
            delay_config_hash=self._hash_delay_config(),
            backfill_strategy_hash=self._hash_backfill_config(),
            file_path=str(snapshot_path),
            content_hash=content_hash,
            row_count=len(df),
            column_count=len(df.columns),
        )
        self.db.save_snapshot(snapshot)

        logger.info(f"Created snapshot: {snapshot_id} ({len(df)} rows)")
        return snapshot_id

    def load_snapshot(self, snapshot_id: str) -> pd.DataFrame:
        """åŠ è½½å†å²å¿«ç…§ - ç¡®ä¿æ•°æ®ä¸å¯å˜"""
        snapshot = self.db.get_snapshot(snapshot_id)
        df = pd.read_parquet(snapshot.file_path)

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        current_hash = hashlib.sha256(
            pd.util.hash_pandas_object(df).values.tobytes()
        ).hexdigest()[:16]

        if current_hash != snapshot.content_hash:
            raise DataIntegrityError(
                f"Snapshot {snapshot_id} has been corrupted! "
                f"Expected hash: {snapshot.content_hash}, Got: {current_hash}"
            )

        return df

    def record_experiment(self,
                         snapshot_id: str,
                         feature_set_id: str,
                         model_config: dict,
                         train_metrics: dict,
                         test_metrics: dict) -> str:
        """è®°å½•å®éªŒ - å®Œæ•´è¡€ç¼˜é“¾"""

        experiment = ExperimentRecord(
            experiment_id=f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid4().hex[:8]}",
            data_snapshot_id=snapshot_id,
            feature_set_id=feature_set_id,
            model_config_hash=hashlib.sha256(
                json.dumps(model_config, sort_keys=True).encode()
            ).hexdigest()[:16],
            random_seed=model_config.get('random_seed', 42),
            train_metrics=train_metrics,
            test_metrics=test_metrics,
            git_commit=self._get_git_commit(),
            created_at=datetime.now(timezone.utc),
        )

        self.db.save_experiment(experiment)
        return experiment.experiment_id
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] create_snapshot() å¯æ­£å¸¸åˆ›å»ºå¿«ç…§
- [ ] load_snapshot() å¯åŠ è½½å¹¶éªŒè¯å¿«ç…§å®Œæ•´æ€§
- [ ] record_experiment() å¯è®°å½•å®Œæ•´è¡€ç¼˜é“¾
- [ ] å¿«ç…§æ•°æ®ä¸å¯å˜æµ‹è¯•é€šè¿‡

---

### 12.5 Step 4: å›æµ‹-å®ç›˜æˆäº¤å¯¹é½

**ç›®æ ‡**: ç¡®ä¿å›æµ‹çš„ fill_price, fee_model, slippage_model ä¸å®ç›˜ä¸€è‡´ã€‚

**æ–‡ä»¶ä½ç½®**: `algvex/core/backtest/execution_model.py`

**å…³é”®å¯¹é½é¡¹**:

```python
# algvex/core/backtest/execution_model.py

class ExecutionModel:
    """ç»Ÿä¸€æˆäº¤æ¨¡å‹ - å›æµ‹å’Œå®ç›˜å…±ç”¨"""

    def __init__(self, config: ExecutionConfig):
        self.config = config
        self.fee_model = FeeModel(config.vip_level)
        self.slippage_model = DynamicSlippageModel()

    def calculate_fill_price(self,
                            side: str,
                            order_type: str,
                            market_data: dict) -> float:
        """
        è®¡ç®—æˆäº¤ä»·æ ¼

        è§„åˆ™ (å›æµ‹å’Œå®ç›˜å¿…é¡»ä¸€è‡´):
        - MARKET å•: last_price + slippage
        - LIMIT å•: limit_price (å‡è®¾å®Œå…¨æˆäº¤)
        """
        if order_type == "MARKET":
            base_price = market_data['last_price']
            slippage = self.slippage_model.estimate(
                symbol=market_data['symbol'],
                order_size_usd=market_data['order_size_usd'],
                conditions=market_data,
            )
            if side == "BUY":
                return base_price * (1 + slippage)
            else:
                return base_price * (1 - slippage)
        else:
            return market_data['limit_price']

    def calculate_fee(self,
                     notional: float,
                     is_maker: bool) -> float:
        """è®¡ç®—æ‰‹ç»­è´¹"""
        return notional * self.fee_model.get_fee(is_maker)

# ç¡®ä¿å›æµ‹å¼•æ“ä½¿ç”¨ç›¸åŒçš„æˆäº¤æ¨¡å‹
class CryptoPerpetualBacktest:
    def __init__(self, config: BacktestConfig):
        # ä½¿ç”¨ç»Ÿä¸€çš„æˆäº¤æ¨¡å‹
        self.execution_model = ExecutionModel(config.execution_config)

# ç¡®ä¿å®ç›˜æ¡¥æ¥å™¨ä½¿ç”¨ç›¸åŒçš„æˆäº¤æ¨¡å‹
class HummingbotBridge:
    def __init__(self, config: LiveConfig):
        # ä½¿ç”¨ç›¸åŒçš„æˆäº¤æ¨¡å‹è¿›è¡Œé¢„ä¼°
        self.execution_model = ExecutionModel(config.execution_config)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] ExecutionModel ç±»å®ç°å®Œæˆ
- [ ] BacktestEngine å’Œ LiveEngine ä½¿ç”¨åŒä¸€ä¸ª ExecutionModel
- [ ] æˆäº¤ä»·æ ¼å¯¹é½æµ‹è¯•é€šè¿‡
- [ ] æ‰‹ç»­è´¹å¯¹é½æµ‹è¯•é€šè¿‡

---

### 12.6 Step 5: Walk-Forward éªŒè¯æµç¨‹

**ç›®æ ‡**: å®ç° Walk-Forward éªŒè¯ï¼Œç¦æ­¢éšæœºåˆ‡åˆ†æ—¶åºæ•°æ®ã€‚

**æ–‡ä»¶ä½ç½®**: `algvex/core/model/validation.py`

```python
# algvex/core/model/validation.py

class WalkForwardValidator:
    """Walk-Forward éªŒè¯å™¨"""

    def __init__(self,
                 train_months: int = 12,
                 test_months: int = 3,
                 min_train_samples: int = 1000,
                 purge_days: int = 7):  # è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹é—´çš„éš”ç¦»å¤©æ•°
        self.train_months = train_months
        self.test_months = test_months
        self.min_train_samples = min_train_samples
        self.purge_days = purge_days

    def create_folds(self, data: pd.DataFrame) -> List[WalkForwardFold]:
        """åˆ›å»º Walk-Forward æŠ˜å """
        folds = []
        # ... å®ç°é€»è¾‘ (å·²åœ¨ P0-4 ä¸­å®šä¹‰)
        return folds

    def validate(self,
                model_class,
                model_params: dict,
                data: pd.DataFrame,
                target_col: str) -> WalkForwardResult:
        """æ‰§è¡Œ Walk-Forward éªŒè¯"""
        folds = self.create_folds(data)
        results = []

        for fold in folds:
            # è®­ç»ƒ
            model = model_class(**model_params)
            model.fit(fold.train_data, fold.train_data[target_col])

            # é¢„æµ‹
            predictions = model.predict(fold.test_data)

            # è®¡ç®—æŒ‡æ ‡
            metrics = self._calculate_metrics(
                fold.test_data[target_col],
                predictions,
            )
            results.append(metrics)

        return WalkForwardResult(
            folds=folds,
            metrics=results,
            aggregate=self._aggregate_metrics(results),
        )

# å¼ºåˆ¶ç¦æ­¢éšæœºåˆ‡åˆ†
def train_test_split(*args, shuffle=False, **kwargs):
    """é‡å†™ train_test_splitï¼Œç¦æ­¢ shuffle=True"""
    if shuffle:
        raise ValueError(
            "ç¦æ­¢éšæœºåˆ‡åˆ†æ—¶åºæ•°æ®ï¼è¯·ä½¿ç”¨ WalkForwardValidatorã€‚"
        )
    return sklearn_train_test_split(*args, shuffle=False, **kwargs)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] WalkForwardValidator å®ç°å®Œæˆ
- [ ] ç¦æ­¢ shuffle=True çš„ä¿æŠ¤é€»è¾‘ç”Ÿæ•ˆ
- [ ] Walk-Forward ç»“æœå¯è§†åŒ–æŠ¥å‘Š

---

### 12.7 Step 6: P0 å•å…ƒæµ‹è¯•

**ç›®æ ‡**: ä¸ºæ‰€æœ‰ P0 æ ‡å‡†ç¼–å†™å•å…ƒæµ‹è¯•ã€‚

**æ–‡ä»¶ä½ç½®**: `tests/p0/`

```
tests/p0/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_p0_1_data_visibility.py     # æ•°æ®å¯è§æ€§æµ‹è¯•
â”œâ”€â”€ test_p0_2_price_semantics.py     # ä»·æ ¼è¯­ä¹‰æµ‹è¯•
â”œâ”€â”€ test_p0_3_order_consistency.py   # è®¢å•ä¸€è‡´æ€§æµ‹è¯•
â”œâ”€â”€ test_p0_4_walk_forward.py        # Walk-Forward æµ‹è¯•
â”œâ”€â”€ test_p0_5_data_lineage.py        # æ•°æ®è¡€ç¼˜æµ‹è¯•
â”œâ”€â”€ test_p0_6_execution_alignment.py # æˆäº¤å¯¹é½æµ‹è¯•
â””â”€â”€ conftest.py                      # pytest fixtures
```

```python
# tests/p0/test_p0_1_data_visibility.py

import pytest
from datetime import datetime, timezone, timedelta

class TestP0_1_DataVisibility:
    """P0-1: æ•°æ®å¯è§æ€§æµ‹è¯•"""

    def test_bar_aggregated_visibility(self, data_manager):
        """æµ‹è¯• bar èšåˆç‰¹å¾çš„å¯è§æ€§"""
        signal_time = datetime(2024, 6, 15, 12, 0, 0, tzinfo=timezone.utc)

        # è·å– CVD æ•°æ®
        cvd = data_manager.get_cvd_at_time(signal_time, bar_freq="1h")

        # CVD åº”è¯¥ä½¿ç”¨ 11:00 bar çš„æ•°æ®ï¼Œä¸èƒ½ä½¿ç”¨ 12:00 bar
        assert cvd.index.max() <= signal_time - timedelta(hours=1), \
            "CVD ä½¿ç”¨äº†æœªæ”¶ç›˜ bar çš„æ•°æ®ï¼Œå­˜åœ¨æ³„éœ²ï¼"

    def test_no_future_leakage(self, data_manager, sample_signals):
        """æµ‹è¯•æ— æœªæ¥æ•°æ®æ³„éœ²"""
        for signal in sample_signals:
            features = data_manager.get_features_at_time(signal.time)
            for f in features:
                visible_time = data_manager.get_visible_time(f)
                assert visible_time <= signal.time, \
                    f"å‘ç°æ³„éœ²: {f.name} visible_time > signal_time"

    def test_merge_asof_used(self):
        """æµ‹è¯•æ˜¯å¦ä½¿ç”¨ merge_asof è€Œéæ™®é€š merge"""
        import ast
        from pathlib import Path

        # æ‰«ææ‰€æœ‰å› å­è®¡ç®—å’Œæ•°æ®åˆå¹¶ç›¸å…³ä»£ç 
        target_dirs = [
            Path("algvex/core/factor"),
            Path("algvex/core/data"),
        ]

        violations = []
        for target_dir in target_dirs:
            if not target_dir.exists():
                continue

            for py_file in target_dir.rglob("*.py"):
                content = py_file.read_text()
                tree = ast.parse(content)

                for node in ast.walk(tree):
                    # æ£€æŸ¥ pd.merge() è°ƒç”¨
                    if isinstance(node, ast.Call):
                        func = node.func
                        # æ£€æŸ¥ pd.merge æˆ– DataFrame.merge
                        if isinstance(func, ast.Attribute):
                            if func.attr == "merge":
                                # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶åºæ•°æ®åˆå¹¶åœºæ™¯
                                # merge_asof çš„ç‰¹å¾: direction å‚æ•°
                                has_direction = any(
                                    kw.arg == "direction"
                                    for kw in node.keywords
                                )
                                if not has_direction:
                                    # æ£€æŸ¥æ³¨é‡Šæ˜¯å¦æœ‰è±å…æ ‡è®°
                                    violations.append({
                                        "file": str(py_file),
                                        "line": node.lineno,
                                        "issue": "ä½¿ç”¨ merge è€Œé merge_asof",
                                    })

        # æŠ¥å‘Šç»“æœ
        if violations:
            for v in violations:
                # å…è®¸é€šè¿‡ # noqa: ASOF è±å…
                print(f"âš ï¸ {v['file']}:{v['line']} - {v['issue']}")
                print("   è¯·ç¡®è®¤æ˜¯å¦éœ€è¦æ”¹ä¸º merge_asof (æ—¶åºæ•°æ®åˆå¹¶åœºæ™¯)")

        # è‡³å°‘æ£€æŸ¥æ ¸å¿ƒæ–‡ä»¶å­˜åœ¨ merge_asof è°ƒç”¨
        core_files = list(Path("algvex/core/factor").rglob("*.py"))
        has_merge_asof = False
        for f in core_files:
            if "merge_asof" in f.read_text():
                has_merge_asof = True
                break

        assert has_merge_asof, "æ ¸å¿ƒå› å­æ¨¡å—å¿…é¡»ä½¿ç”¨ merge_asof è¿›è¡Œæ—¶åºæ•°æ®åˆå¹¶"
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ¯ä¸ª P0 æ ‡å‡†è‡³å°‘æœ‰ 3 ä¸ªæµ‹è¯•ç”¨ä¾‹
- [ ] æµ‹è¯•è¦†ç›–ç‡ > 80%
- [ ] æ‰€æœ‰ P0 æµ‹è¯•é€šè¿‡

---

### 12.8 Step 7: æ•°æ®è´¨é‡ç›‘æ§ (è¡¥å……)

**ç›®æ ‡**: ç›‘æ§æ•°æ®æºå¥åº·çŠ¶æ€ï¼ŒåŠæ—¶å‘ç°é—®é¢˜ã€‚

**æ–‡ä»¶ä½ç½®**: `algvex/core/data/quality.py`

```python
# algvex/core/data/quality.py

class DataQualityMonitor:
    """æ•°æ®è´¨é‡ç›‘æ§å™¨"""

    # ç›‘æ§æŒ‡æ ‡é˜ˆå€¼
    THRESHOLDS = {
        "missing_rate": 0.05,       # ç¼ºå¤±ç‡ > 5% å‘Šè­¦
        "delay_seconds": 300,       # å»¶è¿Ÿ > 5åˆ†é’Ÿå‘Šè­¦
        "schema_change": True,      # å­—æ®µå˜åŒ–å‘Šè­¦
        "value_range_violation": 0.01,  # å¼‚å¸¸å€¼ > 1% å‘Šè­¦
    }

    def check_data_source(self, source: str) -> DataQualityReport:
        """æ£€æŸ¥å•ä¸ªæ•°æ®æºçš„å¥åº·çŠ¶æ€"""
        report = DataQualityReport(source=source)

        # 1. æ£€æŸ¥æœ€æ–°æ•°æ®æ—¶é—´ (å»¶è¿Ÿæ£€æµ‹)
        latest_time = self.db.get_latest_time(source)
        delay = datetime.now(timezone.utc) - latest_time
        if delay.total_seconds() > self.THRESHOLDS["delay_seconds"]:
            report.add_alert(
                level="WARNING",
                message=f"Data delay: {delay.total_seconds()}s",
            )

        # 2. æ£€æŸ¥ç¼ºå¤±ç‡
        missing_rate = self.db.get_missing_rate(source, window="24h")
        if missing_rate > self.THRESHOLDS["missing_rate"]:
            report.add_alert(
                level="ERROR",
                message=f"High missing rate: {missing_rate:.2%}",
            )

        # 3. æ£€æŸ¥å­—æ®µå˜åŒ–
        current_schema = self.get_current_schema(source)
        expected_schema = self.get_expected_schema(source)
        if current_schema != expected_schema:
            report.add_alert(
                level="CRITICAL",
                message=f"Schema changed: {current_schema}",
            )

        return report

    def run_all_checks(self) -> List[DataQualityReport]:
        """è¿è¡Œæ‰€æœ‰æ•°æ®æºæ£€æŸ¥"""
        reports = []
        for source in self.ALL_SOURCES:
            report = self.check_data_source(source)
            reports.append(report)

            # å‘é€å‘Šè­¦
            if report.has_critical():
                self.alert_manager.send_critical(report)
            elif report.has_error():
                self.alert_manager.send_error(report)

        return reports
```

**Celery å®šæ—¶æ£€æŸ¥**:

```python
@app.task
def check_data_quality():
    """æ¯å°æ—¶æ£€æŸ¥æ•°æ®è´¨é‡"""
    monitor = DataQualityMonitor()
    reports = monitor.run_all_checks()

    # ç”ŸæˆæŠ¥å‘Š
    summary = DataQualitySummary(reports)
    logger.info(f"Data quality check: {summary.status}")

    # å¦‚æœæœ‰ä¸¥é‡é—®é¢˜ï¼Œæš‚åœç›¸å…³æ•°æ®é‡‡é›†
    if summary.has_critical():
        pause_data_collection(summary.critical_sources)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ•°æ®å»¶è¿Ÿç›‘æ§æ­£å¸¸
- [ ] ç¼ºå¤±ç‡ç›‘æ§æ­£å¸¸
- [ ] å‘Šè­¦é€šçŸ¥å¯è¾¾ (Slack/é‚®ä»¶)

---

### 12.9 Step 8: CI/CD é›†æˆ (è¡¥å……)

**ç›®æ ‡**: å°† P0 æµ‹è¯•é›†æˆåˆ° CI/CD æµç¨‹ï¼Œç¡®ä¿æ¯æ¬¡æäº¤éƒ½é€šè¿‡éªŒæ”¶ã€‚

**GitHub Actions é…ç½®**:

```yaml
# .github/workflows/p0-tests.yml

name: P0 Verification Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  p0-tests:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: timescale/timescaledb:latest-pg15
        env:
          POSTGRES_DB: algvex_test
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Run P0 Tests
        run: |
          pytest tests/p0/ -v --tb=short --cov=algvex --cov-report=xml

      - name: Check P0 Coverage
        run: |
          # P0 æµ‹è¯•è¦†ç›–ç‡å¿…é¡» > 80%
          coverage report --fail-under=80

      - name: Upload Coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] GitHub Actions é…ç½®å®Œæˆ
- [ ] PR å¿…é¡»é€šè¿‡ P0 æµ‹è¯•æ‰èƒ½åˆå¹¶
- [ ] è¦†ç›–ç‡æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆ

---

### 12.10 å¼€å‘æ£€æŸ¥æ¸…å•

| Phase | Step | æè¿° | çŠ¶æ€ | è´Ÿè´£äºº |
|-------|------|------|------|--------|
| 0 | 1 | æ•°æ®é‡‡é›†å™¨å®ç° (5ä¸ª Collector) | â¬œ | - |
| 0 | 2 | B/Cæ¡£æ•°æ®è½ç›˜ (Celery + TimescaleDB) | â¬œ | - |
| 0 | 3 | æ•°æ®è¡€ç¼˜ä¸å¿«ç…§ | â¬œ | - |
| 0 | 7 | æ•°æ®è´¨é‡ç›‘æ§ | â¬œ | - |
| 1 | 4 | å›æµ‹-å®ç›˜æˆäº¤å¯¹é½ (DynamicSlippageModel) | â¬œ | - |
| 1 | 5 | Walk-Forward éªŒè¯æµç¨‹ | â¬œ | - |
| **1** | **9** | **â˜… L2æ·±åº¦èšåˆ + æ»‘ç‚¹æ ¡å‡†** | â¬œ | - |
| **1** | **10** | **â˜… æ¸…ç®—æ•°æ® + çº§è”æ£€æµ‹** | â¬œ | - |
| **1** | **11** | **â˜… å¤šäº¤æ˜“æ‰€Basis (Binance/Bybit/OKX)** | â¬œ | - |
| 2 | 6 | P0 å•å…ƒæµ‹è¯• (6ç»„) | â¬œ | - |
| 2 | 8 | CI/CD é›†æˆ | â¬œ | - |
| 3 | - | Baselineæ¨¡å‹è®­ç»ƒ (30-60ç¨³å®šå› å­) | â¬œ | - |
| 3 | - | å®ç›˜å½±å­æ¨¡å¼ (Paper Trading) | â¬œ | - |

---

### 12.11 Step 9: L2 æ·±åº¦èšåˆ + æ»‘ç‚¹æ¨¡å‹æ ¡å‡† (ä¼˜å…ˆå®æ–½)

> **ä¸ºä»€ä¹ˆä¼˜å…ˆ**: è¿™æ˜¯ç¬¬ä¸€ä¸ªæ•°æ®æ‰©å±•ï¼Œç›´æ¥è§£å†³ P0-6 æ»‘ç‚¹æ¨¡å‹"ä¼°ç®—"çš„é—®é¢˜ï¼Œç”¨çœŸå®æ·±åº¦æ•°æ®æ ¡å‡†ã€‚
>
> **å·¥ç¨‹å¤æ‚åº¦**: å¯æ§ã€‚åªåš 1m/5m bar èšåˆï¼Œä¸åšæ¯«ç§’çº§ orderbook å¿«ç…§ã€‚
>
> **åŒå‘å—ç›Š**: å›æµ‹æ»‘ç‚¹æ›´çœŸå® + å®ç›˜ä¸‹å•å‰å¯é¢„ä¼°å†²å‡»æˆæœ¬ã€‚

#### 12.11.1 æ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Step 9: L2 æ·±åº¦èšåˆ + æ»‘ç‚¹æ ¡å‡† æ¶æ„                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    1. DepthCollector (WebSocket)                    â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚   Binance WS â”€â”€â†’ åŸå§‹æ·±åº¦å¿«ç…§ â”€â”€â†’ 1m/5m èšåˆ â”€â”€â†’ TimescaleDB       â”‚   â”‚
â”‚  â”‚   (100msæ›´æ–°)     (å†…å­˜buffer)      (bar_close)    (æŒä¹…åŒ–)         â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚   âš ï¸ å¯è§æ€§: bar_close (åªæœ‰å½“barç»“æŸåï¼Œèšåˆæ•°æ®æ‰å¯ç”¨)              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â†“                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    2. æ·±åº¦å› å­è®¡ç®— (8ä¸ªæ ¸å¿ƒæŒ‡æ ‡)                      â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚   bid_ask_spread, order_book_imbalance, depth_1pct_bid/ask,         â”‚   â”‚
â”‚  â”‚   depth_slope_bid/ask, impact_cost_buy/sell                         â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â†“                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    3. CalibratedSlippageModel                        â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚   Step 4 DynamicSlippageModel (ä¼°ç®—) â”€â”€å‡çº§â”€â”€â†’ çœŸå®æ·±åº¦æ ¡å‡†           â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚   å›æµ‹: ç”¨å†å² impact_cost ä»£æ›¿ç»éªŒå…¬å¼                               â”‚   â”‚
â”‚  â”‚   å®ç›˜: ç”¨å®æ—¶æ·±åº¦é¢„ä¼°ä¸‹å•å†²å‡»                                        â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 12.11.2 æ–‡ä»¶ç»“æ„

```
algvex/core/data/collectors/
â”œâ”€â”€ depth.py                    # DepthCollector (æ–°å¢)
â”‚
algvex/core/data/features/
â”œâ”€â”€ depth_features.py           # 8ä¸ªæ·±åº¦å› å­è®¡ç®— (æ–°å¢)
â”‚
algvex/core/backtest/
â”œâ”€â”€ slippage_model.py           # DynamicSlippageModel (å·²æœ‰, Step 4)
â”œâ”€â”€ calibrated_slippage.py      # CalibratedSlippageModel (æ–°å¢, Step 9)
â”‚
tests/p0/
â”œâ”€â”€ test_depth_collector.py     # æ·±åº¦é‡‡é›†æµ‹è¯• (æ–°å¢)
â”œâ”€â”€ test_calibrated_slippage.py # æ ¡å‡†æ»‘ç‚¹æµ‹è¯• (æ–°å¢)
```

#### 12.11.3 DepthCollector å®ç°

**æ–‡ä»¶ä½ç½®**: `algvex/core/data/collectors/depth.py`

```python
# algvex/core/data/collectors/depth.py

import asyncio
import json
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Dict, List, Optional, Callable
from collections import defaultdict
import numpy as np
import pandas as pd
import websockets

from .base import BaseCollector


@dataclass
class DepthSnapshot:
    """å•æ¬¡æ·±åº¦å¿«ç…§"""
    timestamp: datetime
    symbol: str
    bids: List[List[float]]  # [[price, qty], ...]
    asks: List[List[float]]  # [[price, qty], ...]

    @property
    def best_bid(self) -> float:
        return self.bids[0][0] if self.bids else 0

    @property
    def best_ask(self) -> float:
        return self.asks[0][0] if self.asks else float('inf')

    @property
    def mid_price(self) -> float:
        return (self.best_bid + self.best_ask) / 2


@dataclass
class AggregatedDepthBar:
    """èšåˆåçš„æ·±åº¦ Bar (1m/5m)"""
    bar_time: datetime           # bar å¼€å§‹æ—¶é—´
    symbol: str

    # èšåˆç»Ÿè®¡ (bar å†…æ‰€æœ‰å¿«ç…§çš„å‡å€¼/åŠ æƒå‡å€¼)
    avg_bid_ask_spread: float    # å¹³å‡ spread (bps)
    avg_imbalance: float         # å¹³å‡ imbalance (-1 to 1)
    avg_depth_1pct_bid: float    # 1% èŒƒå›´å†…å¹³å‡ä¹°å•é‡ (USD)
    avg_depth_1pct_ask: float    # 1% èŒƒå›´å†…å¹³å‡å–å•é‡ (USD)
    avg_depth_slope_bid: float   # ä¹°å•é‡è¡°å‡æ–œç‡
    avg_depth_slope_ask: float   # å–å•é‡è¡°å‡æ–œç‡

    # å†²å‡»æˆæœ¬ (å…³é”®! ç”¨äºæ»‘ç‚¹æ ¡å‡†)
    impact_cost_10k_buy: float   # ä¹°å…¥ $10k çš„å†²å‡»æˆæœ¬ (bps)
    impact_cost_10k_sell: float  # å–å‡º $10k çš„å†²å‡»æˆæœ¬ (bps)
    impact_cost_50k_buy: float   # ä¹°å…¥ $50k çš„å†²å‡»æˆæœ¬ (bps)
    impact_cost_50k_sell: float  # å–å‡º $50k çš„å†²å‡»æˆæœ¬ (bps)
    impact_cost_100k_buy: float  # ä¹°å…¥ $100k çš„å†²å‡»æˆæœ¬ (bps)
    impact_cost_100k_sell: float # å–å‡º $100k çš„å†²å‡»æˆæœ¬ (bps)

    # å…ƒæ•°æ®
    snapshot_count: int          # bar å†…é‡‡é›†çš„å¿«ç…§æ•°
    visibility: str = "bar_close"  # å¯è§æ€§è§„åˆ™


class DepthCollector(BaseCollector):
    """
    å¸å®‰ L2 æ·±åº¦é‡‡é›†å™¨ (WebSocket)

    âš ï¸ å¯è§æ€§è§„åˆ™: bar_close
    - æ·±åº¦æ•°æ®åœ¨ bar ç»“æŸåæ‰èƒ½ç”¨äºå› å­è®¡ç®—
    - é˜²æ­¢æœªæ¥ä¿¡æ¯æ³„éœ²

    å­˜å‚¨ç­–ç•¥: åªå­˜èšåˆåçš„ bar æ•°æ®ï¼Œä¸å­˜åŸå§‹å¿«ç…§ (å¤ªå¤§)
    """

    # å¸å®‰ WebSocket é…ç½®
    WS_URL = "wss://fstream.binance.com/ws"
    DEPTH_LEVELS = 20  # å‰20æ¡£
    UPDATE_SPEED = "100ms"  # 100ms æ›´æ–°é¢‘ç‡

    # èšåˆé…ç½®
    BAR_FREQUENCIES = ["1m", "5m"]  # æ”¯æŒçš„èšåˆå‘¨æœŸ

    # å†²å‡»æˆæœ¬è®¡ç®—çš„è®¢å•è§„æ¨¡ (USD)
    IMPACT_SIZES = [10_000, 50_000, 100_000]

    def __init__(self,
                 symbols: List[str],
                 bar_freq: str = "1m",
                 on_bar_complete: Optional[Callable] = None):
        """
        Args:
            symbols: è¦è®¢é˜…çš„äº¤æ˜“å¯¹åˆ—è¡¨ (å¦‚ ["btcusdt", "ethusdt"])
            bar_freq: èšåˆé¢‘ç‡ ("1m" æˆ– "5m")
            on_bar_complete: bar å®Œæˆæ—¶çš„å›è°ƒå‡½æ•°
        """
        self.symbols = [s.lower() for s in symbols]
        self.bar_freq = bar_freq
        self.on_bar_complete = on_bar_complete

        # å†…å­˜ç¼“å†²åŒº: symbol -> å½“å‰ bar çš„å¿«ç…§åˆ—è¡¨
        self._buffers: Dict[str, List[DepthSnapshot]] = defaultdict(list)

        # å½“å‰ bar å¼€å§‹æ—¶é—´
        self._current_bar_start: Dict[str, datetime] = {}

        # è¿è¡ŒçŠ¶æ€
        self._running = False
        self._ws = None

    async def start(self):
        """å¯åŠ¨ WebSocket è¿æ¥å’Œæ•°æ®é‡‡é›†"""
        self._running = True

        # æ„å»ºè®¢é˜… streams
        streams = [f"{s}@depth{self.DEPTH_LEVELS}@{self.UPDATE_SPEED}"
                   for s in self.symbols]
        url = f"{self.WS_URL}/stream?streams={'/'.join(streams)}"

        while self._running:
            try:
                async with websockets.connect(url) as ws:
                    self._ws = ws
                    await self._receive_loop()
            except Exception as e:
                if self._running:
                    print(f"WebSocket disconnected: {e}, reconnecting in 5s...")
                    await asyncio.sleep(5)

    async def _receive_loop(self):
        """æ¥æ”¶å¹¶å¤„ç†æ·±åº¦æ›´æ–°"""
        async for message in self._ws:
            data = json.loads(message)

            # è§£ææ·±åº¦æ•°æ®
            stream = data.get("stream", "")
            symbol = stream.split("@")[0]
            depth_data = data.get("data", {})

            snapshot = DepthSnapshot(
                timestamp=datetime.now(timezone.utc),
                symbol=symbol,
                bids=[[float(p), float(q)] for p, q in depth_data.get("b", [])],
                asks=[[float(p), float(q)] for p, q in depth_data.get("a", [])],
            )

            # æ·»åŠ åˆ°ç¼“å†²åŒº
            self._add_to_buffer(snapshot)

    def _add_to_buffer(self, snapshot: DepthSnapshot):
        """æ·»åŠ å¿«ç…§åˆ°ç¼“å†²åŒºï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦èšåˆ"""
        symbol = snapshot.symbol

        # è®¡ç®—å½“å‰ bar å¼€å§‹æ—¶é—´
        bar_start = self._get_bar_start(snapshot.timestamp)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å®Œæˆä¸Šä¸€ä¸ª bar
        if symbol in self._current_bar_start:
            if bar_start > self._current_bar_start[symbol]:
                # å®Œæˆä¸Šä¸€ä¸ª bar
                self._complete_bar(symbol, self._current_bar_start[symbol])
                self._buffers[symbol] = []

        self._current_bar_start[symbol] = bar_start
        self._buffers[symbol].append(snapshot)

    def _get_bar_start(self, ts: datetime) -> datetime:
        """è®¡ç®— bar å¼€å§‹æ—¶é—´"""
        if self.bar_freq == "1m":
            return ts.replace(second=0, microsecond=0)
        elif self.bar_freq == "5m":
            minute = (ts.minute // 5) * 5
            return ts.replace(minute=minute, second=0, microsecond=0)
        else:
            raise ValueError(f"Unsupported bar_freq: {self.bar_freq}")

    def _complete_bar(self, symbol: str, bar_time: datetime):
        """èšåˆå¹¶è¾“å‡ºä¸€ä¸ªå®Œæ•´çš„ bar"""
        snapshots = self._buffers[symbol]
        if not snapshots:
            return

        # è®¡ç®—èšåˆæŒ‡æ ‡
        aggregated = self._aggregate_snapshots(symbol, bar_time, snapshots)

        # å›è°ƒ
        if self.on_bar_complete:
            self.on_bar_complete(aggregated)

    def _aggregate_snapshots(self,
                             symbol: str,
                             bar_time: datetime,
                             snapshots: List[DepthSnapshot]) -> AggregatedDepthBar:
        """èšåˆå¿«ç…§ä¸º bar æ•°æ®"""

        spreads = []
        imbalances = []
        depth_1pct_bids = []
        depth_1pct_asks = []
        slope_bids = []
        slope_asks = []
        impact_costs = {size: {"buy": [], "sell": []}
                       for size in self.IMPACT_SIZES}

        for snap in snapshots:
            mid = snap.mid_price
            if mid <= 0:
                continue

            # 1. Bid-Ask Spread (bps)
            spread_bps = (snap.best_ask - snap.best_bid) / mid * 10000
            spreads.append(spread_bps)

            # 2. Order Book Imbalance
            bid_qty = sum(qty for _, qty in snap.bids)
            ask_qty = sum(qty for _, qty in snap.asks)
            total = bid_qty + ask_qty
            imbalance = (bid_qty - ask_qty) / total if total > 0 else 0
            imbalances.append(imbalance)

            # 3. Depth within 1% (USD)
            depth_bid = self._calculate_depth_within_pct(snap.bids, mid, 0.01)
            depth_ask = self._calculate_depth_within_pct(snap.asks, mid, 0.01)
            depth_1pct_bids.append(depth_bid * mid)  # è½¬æ¢ä¸º USD
            depth_1pct_asks.append(depth_ask * mid)

            # 4. Depth Slope (è¡°å‡é€Ÿåº¦)
            slope_bid = self._calculate_depth_slope(snap.bids, mid)
            slope_ask = self._calculate_depth_slope(snap.asks, mid)
            slope_bids.append(slope_bid)
            slope_asks.append(slope_ask)

            # 5. Impact Cost (å…³é”®!)
            for size in self.IMPACT_SIZES:
                cost_buy = self._calculate_impact_cost(snap.asks, mid, size)
                cost_sell = self._calculate_impact_cost(snap.bids, mid, size)
                impact_costs[size]["buy"].append(cost_buy)
                impact_costs[size]["sell"].append(cost_sell)

        return AggregatedDepthBar(
            bar_time=bar_time,
            symbol=symbol.upper(),
            avg_bid_ask_spread=np.mean(spreads) if spreads else 0,
            avg_imbalance=np.mean(imbalances) if imbalances else 0,
            avg_depth_1pct_bid=np.mean(depth_1pct_bids) if depth_1pct_bids else 0,
            avg_depth_1pct_ask=np.mean(depth_1pct_asks) if depth_1pct_asks else 0,
            avg_depth_slope_bid=np.mean(slope_bids) if slope_bids else 0,
            avg_depth_slope_ask=np.mean(slope_asks) if slope_asks else 0,
            impact_cost_10k_buy=np.mean(impact_costs[10000]["buy"]),
            impact_cost_10k_sell=np.mean(impact_costs[10000]["sell"]),
            impact_cost_50k_buy=np.mean(impact_costs[50000]["buy"]),
            impact_cost_50k_sell=np.mean(impact_costs[50000]["sell"]),
            impact_cost_100k_buy=np.mean(impact_costs[100000]["buy"]),
            impact_cost_100k_sell=np.mean(impact_costs[100000]["sell"]),
            snapshot_count=len(snapshots),
        )

    def _calculate_depth_within_pct(self,
                                    levels: List[List[float]],
                                    mid: float,
                                    pct: float) -> float:
        """è®¡ç®—æŒ‡å®šç™¾åˆ†æ¯”èŒƒå›´å†…çš„æ·±åº¦"""
        total_qty = 0
        for price, qty in levels:
            if abs(price - mid) / mid <= pct:
                total_qty += qty
        return total_qty

    def _calculate_depth_slope(self,
                               levels: List[List[float]],
                               mid: float) -> float:
        """è®¡ç®—æ·±åº¦è¡°å‡æ–œç‡ (è¶Šé™¡å³­è¯´æ˜æµåŠ¨æ€§è¶Šé›†ä¸­åœ¨ best price)"""
        if len(levels) < 5:
            return 0

        distances = []
        quantities = []
        for price, qty in levels[:10]:  # å‰10æ¡£
            dist = abs(price - mid) / mid * 100  # ç™¾åˆ†æ¯”è·ç¦»
            distances.append(dist)
            quantities.append(qty)

        if not distances:
            return 0

        # ç®€å•çº¿æ€§å›å½’æ–œç‡
        x = np.array(distances)
        y = np.array(quantities)
        if len(x) > 1 and np.std(x) > 0:
            slope = np.polyfit(x, y, 1)[0]
            return slope
        return 0

    def _calculate_impact_cost(self,
                               levels: List[List[float]],
                               mid: float,
                               order_size_usd: float) -> float:
        """
        è®¡ç®—å†²å‡»æˆæœ¬ (bps)

        æ¨¡æ‹Ÿåƒæ‰è®¢å•ç°¿ï¼Œè®¡ç®—å¹³å‡æˆäº¤ä»·ä¸ mid çš„åç¦»
        """
        remaining_usd = order_size_usd
        total_qty = 0
        total_cost = 0

        for price, qty in levels:
            level_usd = price * qty
            if remaining_usd <= 0:
                break

            fill_usd = min(remaining_usd, level_usd)
            fill_qty = fill_usd / price

            total_qty += fill_qty
            total_cost += fill_qty * price
            remaining_usd -= fill_usd

        if total_qty == 0:
            return 0

        avg_price = total_cost / total_qty
        impact_bps = abs(avg_price - mid) / mid * 10000
        return impact_bps

    async def stop(self):
        """åœæ­¢é‡‡é›†"""
        self._running = False
        if self._ws:
            await self._ws.close()


# ============== TimescaleDB å­˜å‚¨ ==============

DEPTH_TABLE_SCHEMA = """
CREATE TABLE IF NOT EXISTS depth_bars (
    bar_time TIMESTAMPTZ NOT NULL,
    symbol VARCHAR(20) NOT NULL,

    -- æµåŠ¨æ€§æŒ‡æ ‡
    avg_bid_ask_spread DOUBLE PRECISION,
    avg_imbalance DOUBLE PRECISION,
    avg_depth_1pct_bid DOUBLE PRECISION,
    avg_depth_1pct_ask DOUBLE PRECISION,
    avg_depth_slope_bid DOUBLE PRECISION,
    avg_depth_slope_ask DOUBLE PRECISION,

    -- å†²å‡»æˆæœ¬ (æ ¸å¿ƒï¼ç”¨äºæ»‘ç‚¹æ ¡å‡†)
    impact_cost_10k_buy DOUBLE PRECISION,
    impact_cost_10k_sell DOUBLE PRECISION,
    impact_cost_50k_buy DOUBLE PRECISION,
    impact_cost_50k_sell DOUBLE PRECISION,
    impact_cost_100k_buy DOUBLE PRECISION,
    impact_cost_100k_sell DOUBLE PRECISION,

    -- å…ƒæ•°æ®
    snapshot_count INTEGER,

    PRIMARY KEY (bar_time, symbol)
);

-- åˆ›å»º hypertable (TimescaleDB)
SELECT create_hypertable('depth_bars', 'bar_time', if_not_exists => TRUE);

-- ç´¢å¼•
CREATE INDEX IF NOT EXISTS idx_depth_symbol ON depth_bars (symbol, bar_time DESC);
"""
```

#### 12.11.4 CalibratedSlippageModel å®ç°

**æ–‡ä»¶ä½ç½®**: `algvex/core/backtest/calibrated_slippage.py`

```python
# algvex/core/backtest/calibrated_slippage.py

from dataclasses import dataclass
from typing import Optional, Dict
import numpy as np
import pandas as pd

from .slippage_model import DynamicSlippageModel  # Step 4 çš„åŸºç¡€æ¨¡å‹


@dataclass
class SlippageEstimate:
    """æ»‘ç‚¹ä¼°ç®—ç»“æœ"""
    slippage_bps: float          # ä¼°ç®—æ»‘ç‚¹ (bps)
    confidence: str              # "high" / "medium" / "low"
    source: str                  # "depth_data" / "fallback_model"
    details: Dict                # è¯¦ç»†ä¿¡æ¯


class CalibratedSlippageModel:
    """
    æ ¡å‡†æ»‘ç‚¹æ¨¡å‹ - åŸºäºçœŸå® L2 æ·±åº¦æ•°æ®

    å‡çº§è·¯å¾„:
    - Step 4 DynamicSlippageModel: åŸºäºç»éªŒå…¬å¼ä¼°ç®— (fallback)
    - Step 9 CalibratedSlippageModel: åŸºäºçœŸå®æ·±åº¦æ•°æ®æ ¡å‡† (primary)

    ä½¿ç”¨åœºæ™¯:
    - å›æµ‹: ä½¿ç”¨å†å² impact_cost è®¡ç®—æ›´çœŸå®çš„æ»‘ç‚¹
    - å®ç›˜: ä½¿ç”¨å®æ—¶æ·±åº¦æ•°æ®é¢„ä¼°ä¸‹å•å†²å‡»
    """

    # é¢„è®¾çš„è®¢å•è§„æ¨¡æ¡£ä½ (ä¸ DepthCollector å¯¹é½)
    SIZE_TIERS = [10_000, 50_000, 100_000]

    def __init__(self,
                 data_manager,
                 fallback_model: Optional[DynamicSlippageModel] = None):
        """
        Args:
            data_manager: æ•°æ®ç®¡ç†å™¨ (ç”¨äºè·å–æ·±åº¦æ•°æ®)
            fallback_model: å½“æ²¡æœ‰æ·±åº¦æ•°æ®æ—¶çš„å›é€€æ¨¡å‹ (Step 4)
        """
        self.data_manager = data_manager
        self.fallback_model = fallback_model or DynamicSlippageModel()

        # æ ¡å‡†ç³»æ•° (å¯é€šè¿‡å†å²æ•°æ®æ‹Ÿåˆ)
        self.calibration_params = {
            "spread_weight": 0.5,      # spread å¯¹æ»‘ç‚¹çš„è´¡çŒ®
            "impact_weight": 1.0,      # impact_cost å¯¹æ»‘ç‚¹çš„è´¡çŒ®
            "volatility_adj": 0.3,     # æ³¢åŠ¨ç‡è°ƒæ•´ç³»æ•°
        }

    def estimate_slippage(self,
                         symbol: str,
                         order_size_usd: float,
                         bar_time: pd.Timestamp,
                         side: str = "buy",
                         use_fallback_if_missing: bool = True) -> SlippageEstimate:
        """
        ä¼°ç®—æ»‘ç‚¹

        Args:
            symbol: äº¤æ˜“å¯¹ (å¦‚ "BTCUSDT")
            order_size_usd: è®¢å•é‡‘é¢ (USD)
            bar_time: å½“å‰ bar æ—¶é—´ (ç”¨äºè·å– as-of æ·±åº¦æ•°æ®)
            side: "buy" æˆ– "sell"
            use_fallback_if_missing: æ— æ·±åº¦æ•°æ®æ—¶æ˜¯å¦ä½¿ç”¨å›é€€æ¨¡å‹

        Returns:
            SlippageEstimate: æ»‘ç‚¹ä¼°ç®—ç»“æœ
        """

        # 1. å°è¯•è·å–æ·±åº¦æ•°æ®
        depth_data = self._get_depth_at_time(symbol, bar_time)

        if depth_data is None:
            # æ— æ·±åº¦æ•°æ®ï¼Œä½¿ç”¨å›é€€æ¨¡å‹
            if use_fallback_if_missing:
                fallback_slip = self.fallback_model.estimate_slippage(
                    symbol=symbol,
                    order_size_usd=order_size_usd,
                    market_conditions=self._get_market_conditions(symbol, bar_time)
                )
                return SlippageEstimate(
                    slippage_bps=fallback_slip * 10000,  # è½¬ä¸º bps
                    confidence="low",
                    source="fallback_model",
                    details={"reason": "no_depth_data"}
                )
            else:
                raise ValueError(f"No depth data for {symbol} at {bar_time}")

        # 2. æ ¹æ®è®¢å•è§„æ¨¡æ’å€¼è®¡ç®—å†²å‡»æˆæœ¬
        impact_bps = self._interpolate_impact_cost(
            depth_data, order_size_usd, side
        )

        # 3. åŠ å…¥ spread è´¡çŒ®
        spread_bps = depth_data.get("avg_bid_ask_spread", 0)
        spread_contribution = spread_bps * self.calibration_params["spread_weight"]

        # 4. æ³¢åŠ¨ç‡è°ƒæ•´ (é«˜æ³¢åŠ¨æ—¶æ»‘ç‚¹é€šå¸¸æ›´å¤§)
        volatility = self._get_volatility(symbol, bar_time)
        vol_adj = 1 + (volatility - 0.02) * self.calibration_params["volatility_adj"]
        vol_adj = max(0.5, min(vol_adj, 2.0))  # é™åˆ¶åœ¨ [0.5, 2.0]

        # 5. ç»¼åˆè®¡ç®—
        total_slippage_bps = (impact_bps + spread_contribution) * vol_adj

        return SlippageEstimate(
            slippage_bps=total_slippage_bps,
            confidence="high" if depth_data.get("snapshot_count", 0) > 30 else "medium",
            source="depth_data",
            details={
                "impact_bps": impact_bps,
                "spread_contribution": spread_contribution,
                "volatility_adj": vol_adj,
                "snapshot_count": depth_data.get("snapshot_count", 0),
            }
        )

    def _get_depth_at_time(self,
                          symbol: str,
                          bar_time: pd.Timestamp) -> Optional[Dict]:
        """
        è·å–æŒ‡å®šæ—¶é—´çš„æ·±åº¦æ•°æ® (as-of query)

        âš ï¸ å¯è§æ€§è§„åˆ™: åªèƒ½è·å– bar_time ä¹‹å‰å·²å®Œæˆçš„ bar æ•°æ®
        """
        return self.data_manager.get_depth_bar(
            symbol=symbol,
            bar_time=bar_time,
            visibility_rule="bar_close"  # ç¡®ä¿ä¸æ³„éœ²æœªæ¥ä¿¡æ¯
        )

    def _interpolate_impact_cost(self,
                                 depth_data: Dict,
                                 order_size_usd: float,
                                 side: str) -> float:
        """
        æ ¹æ®è®¢å•è§„æ¨¡æ’å€¼è®¡ç®—å†²å‡»æˆæœ¬

        é¢„å­˜çš„æ¡£ä½: 10k, 50k, 100k
        å¯¹äºå…¶ä»–è§„æ¨¡ï¼Œä½¿ç”¨çº¿æ€§/å¯¹æ•°æ’å€¼
        """
        suffix = "buy" if side.lower() == "buy" else "sell"

        # è·å–å„æ¡£ä½çš„å†²å‡»æˆæœ¬
        costs = {
            10_000: depth_data.get(f"impact_cost_10k_{suffix}", 0),
            50_000: depth_data.get(f"impact_cost_50k_{suffix}", 0),
            100_000: depth_data.get(f"impact_cost_100k_{suffix}", 0),
        }

        # å°äº 10k: ç›´æ¥ç”¨ 10k çš„å€¼ (ä¿å®ˆ)
        if order_size_usd <= 10_000:
            # çº¿æ€§ç¼©æ”¾
            return costs[10_000] * (order_size_usd / 10_000)

        # 10k-50k: çº¿æ€§æ’å€¼
        if order_size_usd <= 50_000:
            t = (order_size_usd - 10_000) / (50_000 - 10_000)
            return costs[10_000] + t * (costs[50_000] - costs[10_000])

        # 50k-100k: çº¿æ€§æ’å€¼
        if order_size_usd <= 100_000:
            t = (order_size_usd - 50_000) / (100_000 - 50_000)
            return costs[50_000] + t * (costs[100_000] - costs[50_000])

        # å¤§äº 100k: å¤–æ¨ (å‡è®¾çº¿æ€§å¢é•¿)
        slope = (costs[100_000] - costs[50_000]) / 50_000
        extra = order_size_usd - 100_000
        return costs[100_000] + slope * extra

    def _get_volatility(self, symbol: str, bar_time: pd.Timestamp) -> float:
        """è·å–å½“å‰æ³¢åŠ¨ç‡ (ç”¨äºè°ƒæ•´æ»‘ç‚¹)"""
        # ä» DataManager è·å–æ³¢åŠ¨ç‡å› å­
        try:
            vol = self.data_manager.get_feature(
                symbol=symbol,
                feature="volatility_24h",
                bar_time=bar_time
            )
            return vol if vol else 0.02  # é»˜è®¤ 2%
        except:
            return 0.02

    def _get_market_conditions(self,
                               symbol: str,
                               bar_time: pd.Timestamp) -> Dict:
        """è·å–å¸‚åœºæ¡ä»¶ (ç”¨äº fallback æ¨¡å‹)"""
        return {
            "volatility": self._get_volatility(symbol, bar_time),
            "avg_daily_volume": 1e9,  # é»˜è®¤å€¼
            "bid_ask_spread": 0.0005,  # é»˜è®¤ 5bps
        }

    # ============== æ ¡å‡†æ–¹æ³• ==============

    def calibrate(self,
                 historical_trades: pd.DataFrame,
                 historical_depth: pd.DataFrame) -> Dict:
        """
        ä½¿ç”¨å†å²æˆäº¤æ•°æ®æ ¡å‡†æ¨¡å‹å‚æ•°

        Args:
            historical_trades: å†å²æˆäº¤è®°å½• (åŒ…å«å®é™…æ»‘ç‚¹)
                columns: [symbol, timestamp, side, size_usd, expected_price,
                          actual_avg_price, actual_slippage_bps]
            historical_depth: å†å²æ·±åº¦æ•°æ®
                columns: [symbol, timestamp, bids, asks]

        Returns:
            æ ¡å‡†åçš„å‚æ•°
        """
        import numpy as np
        from scipy import optimize

        calibration_results = {}

        for symbol in historical_trades["symbol"].unique():
            symbol_trades = historical_trades[
                historical_trades["symbol"] == symbol
            ]
            symbol_depth = historical_depth[
                historical_depth["symbol"] == symbol
            ]

            # 1. è®¡ç®—æ¯ç¬”äº¤æ˜“çš„é¢„ä¼°æ»‘ç‚¹ vs å®é™…æ»‘ç‚¹
            errors = []
            for _, trade in symbol_trades.iterrows():
                # æ‰¾åˆ°å¯¹åº”æ—¶é—´ç‚¹çš„æ·±åº¦æ•°æ®
                depth_at_time = symbol_depth[
                    symbol_depth["timestamp"] <= trade["timestamp"]
                ].iloc[-1] if len(symbol_depth) > 0 else None

                if depth_at_time is not None:
                    estimated = self.estimate_slippage_from_depth(
                        size_usd=trade["size_usd"],
                        side=trade["side"],
                        depth=depth_at_time,
                    )
                    actual = trade["actual_slippage_bps"]
                    errors.append(actual - estimated)

            # 2. è®¡ç®—æ ¡å‡†å› å­
            if errors:
                mean_error = np.mean(errors)
                std_error = np.std(errors)

                # æ ¡å‡†å‚æ•°: åç§»é‡ + æ³¢åŠ¨ç‡è°ƒæ•´
                calibration_results[symbol] = {
                    "bias_adjustment_bps": mean_error,
                    "volatility_multiplier": 1 + std_error / 10,
                    "sample_count": len(errors),
                    "calibration_date": pd.Timestamp.now(),
                }

        # 3. æ›´æ–°å†…éƒ¨å‚æ•°
        self.calibration_params.update(calibration_results)

        return calibration_results

    def backtest_vs_actual(self,
                          symbol: str,
                          start_date: str,
                          end_date: str) -> pd.DataFrame:
        """
        å¯¹æ¯”å›æµ‹æ»‘ç‚¹ vs çœŸå®æ·±åº¦æ»‘ç‚¹

        ç”¨äºéªŒè¯æ¨¡å‹å‡†ç¡®æ€§

        Returns:
            DataFrame with columns:
            - timestamp: æ—¶é—´æˆ³
            - size_usd: è®¢å•å¤§å°
            - side: æ–¹å‘
            - backtest_slippage_bps: å›æµ‹ä¼°ç®—æ»‘ç‚¹
            - depth_slippage_bps: çœŸå®æ·±åº¦è®¡ç®—æ»‘ç‚¹
            - error_bps: è¯¯å·®
        """
        # è·å–å†å²æ·±åº¦æ•°æ®
        depth_data = self.data_manager.get_depth_history(
            symbol=symbol,
            start_date=start_date,
            end_date=end_date,
        )

        # æµ‹è¯•ä¸åŒè®¢å•å¤§å°
        test_sizes = [1000, 5000, 10000, 25000, 50000, 100000]
        results = []

        for _, depth_row in depth_data.iterrows():
            for size in test_sizes:
                for side in ["buy", "sell"]:
                    # å›æµ‹æ¨¡å‹ä¼°ç®—
                    backtest_slip = self.fallback_model.estimate_slippage(
                        symbol=symbol,
                        side=side,
                        order_size_usd=size,
                        bar_time=depth_row["timestamp"],
                    )

                    # çœŸå®æ·±åº¦è®¡ç®—
                    depth_slip = self.estimate_slippage_from_depth(
                        size_usd=size,
                        side=side,
                        depth=depth_row,
                    )

                    results.append({
                        "timestamp": depth_row["timestamp"],
                        "size_usd": size,
                        "side": side,
                        "backtest_slippage_bps": backtest_slip * 10000,
                        "depth_slippage_bps": depth_slip,
                        "error_bps": (backtest_slip * 10000) - depth_slip,
                    })

        result_df = pd.DataFrame(results)

        # æ·»åŠ ç»Ÿè®¡æ‘˜è¦
        print(f"=== æ»‘ç‚¹æ¨¡å‹éªŒè¯æŠ¥å‘Š ({symbol}) ===")
        print(f"æ—¶é—´èŒƒå›´: {start_date} ~ {end_date}")
        print(f"æ ·æœ¬æ•°: {len(result_df)}")
        print(f"å¹³å‡è¯¯å·®: {result_df['error_bps'].mean():.2f} bps")
        print(f"è¯¯å·®æ ‡å‡†å·®: {result_df['error_bps'].std():.2f} bps")
        print(f"æœ€å¤§ä½ä¼°: {result_df['error_bps'].min():.2f} bps")
        print(f"æœ€å¤§é«˜ä¼°: {result_df['error_bps'].max():.2f} bps")

        return result_df


# ============== ExecutionModel é›†æˆ ==============

class ExecutionModelV2:
    """
    æ‰§è¡Œæ¨¡å‹ V2 - æ”¯æŒæ ¡å‡†æ»‘ç‚¹

    å‡çº§è·¯å¾„:
    - V1 (Step 4): DynamicSlippageModel (ç»éªŒå…¬å¼)
    - V2 (Step 9): CalibratedSlippageModel (çœŸå®æ·±åº¦)
    """

    def __init__(self,
                 config,
                 use_calibrated_slippage: bool = True):
        self.config = config
        self.fee_model = FeeModel(config.vip_level)

        # æ»‘ç‚¹æ¨¡å‹é€‰æ‹©
        if use_calibrated_slippage:
            self.slippage_model = CalibratedSlippageModel(
                data_manager=config.data_manager,
                fallback_model=DynamicSlippageModel()
            )
        else:
            self.slippage_model = DynamicSlippageModel()

    def calculate_fill_price(self,
                            symbol: str,
                            side: str,
                            order_type: str,
                            order_size_usd: float,
                            bar_time: pd.Timestamp,
                            market_data: dict) -> float:
        """è®¡ç®—æˆäº¤ä»·æ ¼ (è€ƒè™‘çœŸå®æ»‘ç‚¹)"""

        if order_type == "MARKET":
            base_price = market_data['last_price']

            # ä½¿ç”¨æ ¡å‡†æ»‘ç‚¹æ¨¡å‹
            if isinstance(self.slippage_model, CalibratedSlippageModel):
                estimate = self.slippage_model.estimate_slippage(
                    symbol=symbol,
                    order_size_usd=order_size_usd,
                    bar_time=bar_time,
                    side=side,
                )
                slippage = estimate.slippage_bps / 10000  # è½¬ä¸ºå°æ•°
            else:
                slippage = self.slippage_model.estimate_slippage(
                    symbol=symbol,
                    order_size_usd=order_size_usd,
                    market_conditions=market_data,
                )

            if side == "BUY":
                return base_price * (1 + slippage)
            else:
                return base_price * (1 - slippage)
        else:
            return market_data['limit_price']
```

#### 12.11.5 æ•°æ®å¯è§æ€§é…ç½®æ›´æ–°

**æ›´æ–°æ–‡ä»¶**: `algvex/core/data/visibility.py` (Section 11.1 å®šä¹‰çš„)

```python
# æ–°å¢æ·±åº¦æ•°æ®çš„å¯è§æ€§è§„åˆ™
PUBLICATION_DELAYS = {
    # ... å·²æœ‰é…ç½® ...

    # Step 9: L2 æ·±åº¦æ•°æ® (Cæ¡£, bar_close)
    "depth_bid_ask_spread": "bar_close",
    "depth_imbalance": "bar_close",
    "depth_1pct_bid": "bar_close",
    "depth_1pct_ask": "bar_close",
    "depth_slope_bid": "bar_close",
    "depth_slope_ask": "bar_close",
    "depth_impact_cost_buy": "bar_close",
    "depth_impact_cost_sell": "bar_close",
}

# æ•°æ®å¯å¾—æ€§åˆ†çº§
DATA_AVAILABILITY = {
    # ... å·²æœ‰é…ç½® ...

    # Step 9: L2 æ·±åº¦ (Cæ¡£ - å¿…é¡»è‡ªå»ºè½ç›˜)
    "depth_bars": {
        "tier": "C",
        "history_window": "æ—  (å¿…é¡»è‡ªå»º)",
        "schema_stability": "â˜…â˜…â˜†",
        "notes": "WebSocket æ·±åº¦æ•°æ®ï¼Œåªèƒ½å®æ—¶é‡‡é›†ï¼Œæ— å†å² API",
    },
}
```

#### 12.11.6 æµ‹è¯•ç”¨ä¾‹

**æ–‡ä»¶ä½ç½®**: `tests/p0/test_depth_collector.py`

```python
# tests/p0/test_depth_collector.py

import pytest
from datetime import datetime, timezone
from algvex.core.data.collectors.depth import (
    DepthCollector, DepthSnapshot, AggregatedDepthBar
)


class TestDepthSnapshot:
    """æ·±åº¦å¿«ç…§æµ‹è¯•"""

    def test_basic_metrics(self):
        """åŸºç¡€æŒ‡æ ‡è®¡ç®—"""
        snapshot = DepthSnapshot(
            timestamp=datetime.now(timezone.utc),
            symbol="btcusdt",
            bids=[[100000, 1.0], [99900, 2.0], [99800, 3.0]],
            asks=[[100100, 1.0], [100200, 2.0], [100300, 3.0]],
        )

        assert snapshot.best_bid == 100000
        assert snapshot.best_ask == 100100
        assert snapshot.mid_price == 100050

    def test_impact_cost_calculation(self):
        """å†²å‡»æˆæœ¬è®¡ç®—æµ‹è¯•"""
        collector = DepthCollector(symbols=["btcusdt"])

        # æ¨¡æ‹Ÿè®¢å•ç°¿
        asks = [
            [100000, 0.1],   # $10,000
            [100100, 0.2],   # $20,020
            [100200, 0.3],   # $30,060
        ]
        mid = 99950

        # ä¹°å…¥ $10,000 åº”è¯¥åªåƒç¬¬ä¸€æ¡£
        impact_10k = collector._calculate_impact_cost(asks, mid, 10000)
        assert impact_10k < 10  # å°äº 10 bps

        # ä¹°å…¥ $50,000 éœ€è¦åƒæ‰å¤šæ¡£
        impact_50k = collector._calculate_impact_cost(asks, mid, 50000)
        assert impact_50k > impact_10k  # å¤§å•å†²å‡»æ›´å¤§


class TestCalibratedSlippageModel:
    """æ ¡å‡†æ»‘ç‚¹æ¨¡å‹æµ‹è¯•"""

    def test_interpolation(self):
        """å†²å‡»æˆæœ¬æ’å€¼æµ‹è¯•"""
        model = CalibratedSlippageModel(data_manager=MockDataManager())

        # æ¨¡æ‹Ÿæ·±åº¦æ•°æ®
        depth_data = {
            "impact_cost_10k_buy": 2.0,   # 2 bps
            "impact_cost_50k_buy": 5.0,   # 5 bps
            "impact_cost_100k_buy": 10.0, # 10 bps
        }

        # æµ‹è¯•æ’å€¼
        assert model._interpolate_impact_cost(depth_data, 10000, "buy") == 2.0
        assert model._interpolate_impact_cost(depth_data, 30000, "buy") == 3.5  # çº¿æ€§æ’å€¼
        assert model._interpolate_impact_cost(depth_data, 100000, "buy") == 10.0

    def test_fallback_when_no_depth(self):
        """æ— æ·±åº¦æ•°æ®æ—¶å›é€€æµ‹è¯•"""
        model = CalibratedSlippageModel(
            data_manager=MockDataManager(return_none=True),
            fallback_model=DynamicSlippageModel()
        )

        result = model.estimate_slippage(
            symbol="BTCUSDT",
            order_size_usd=10000,
            bar_time=pd.Timestamp.now(),
            use_fallback_if_missing=True
        )

        assert result.source == "fallback_model"
        assert result.confidence == "low"

    def test_visibility_compliance(self):
        """å¯è§æ€§è§„åˆ™åˆè§„æµ‹è¯•"""
        model = CalibratedSlippageModel(data_manager=MockDataManager())

        # ä¸èƒ½ä½¿ç”¨æœªæ¥çš„æ·±åº¦æ•°æ®
        # (MockDataManager åº”è¯¥åªè¿”å› bar_time ä¹‹å‰çš„æ•°æ®)
        # ...
```

#### 12.11.7 éªŒæ”¶æ ‡å‡†

| éªŒæ”¶é¡¹ | æè¿° | æµ‹è¯•æ–¹æ³• | çŠ¶æ€ |
|--------|------|----------|------|
| DepthCollector | WebSocket è¿æ¥ç¨³å®šï¼Œèƒ½æŒç»­é‡‡é›†æ·±åº¦æ•°æ® | è¿è¡Œ 24h æ— æ–­è¿ | â¬œ |
| Bar èšåˆ | 1m/5m èšåˆé€»è¾‘æ­£ç¡®ï¼Œsnapshot_count > 0 | å•å…ƒæµ‹è¯• | â¬œ |
| 8ä¸ªæ·±åº¦å› å­ | æ‰€æœ‰æŒ‡æ ‡è®¡ç®—æ­£ç¡® (spread, imbalance, depth, slope, impact) | å•å…ƒæµ‹è¯• | â¬œ |
| å†²å‡»æˆæœ¬ | impact_cost ä¸çœŸå®è®¢å•ç°¿æ»‘ç‚¹ä¸€è‡´ | å›æ”¾å¯¹æ¯”æµ‹è¯• | â¬œ |
| TimescaleDB å­˜å‚¨ | æ•°æ®æ­£ç¡®å†™å…¥ï¼ŒæŸ¥è¯¢æ€§èƒ½è¾¾æ ‡ | å‹åŠ›æµ‹è¯• | â¬œ |
| CalibratedSlippageModel | æ»‘ç‚¹ä¼°ç®—æ¯” DynamicSlippageModel æ›´å‡†ç¡® | å¯¹æ¯”æµ‹è¯• | â¬œ |
| å¯è§æ€§ | depth æ•°æ®ä½¿ç”¨ bar_close è§„åˆ™ï¼Œæ— æœªæ¥æ³„éœ² | æ³„éœ²æ£€æµ‹æµ‹è¯• | â¬œ |
| Fallback | æ— æ·±åº¦æ•°æ®æ—¶æ­£ç¡®å›é€€åˆ° DynamicSlippageModel | å•å…ƒæµ‹è¯• | â¬œ |
| ExecutionModelV2 | å›æµ‹å¼•æ“æ­£ç¡®ä½¿ç”¨ CalibratedSlippageModel | é›†æˆæµ‹è¯• | â¬œ |
| å®ç›˜é¢„ä¼° | å®ç›˜ä¸‹å•å‰èƒ½é¢„ä¼°å†²å‡»æˆæœ¬ | æ‰‹åŠ¨éªŒè¯ | â¬œ |

---

### 12.12 Step 10: æ¸…ç®—æ•°æ® (Liquidations)

> **å¢é‡ä»·å€¼**: å¯¹"æç«¯è¡Œæƒ…/ç€‘å¸ƒ/æŒ¤ä»“"é¢„æµ‹æ¯”æ™®é€šä»·é‡æ›´æ•æ„Ÿã€‚æ¸…ç®—çº§è”æ˜¯åŠ å¯†å¸‚åœºç‹¬ç‰¹çš„é£é™©ç‰¹å¾ã€‚
>
> **å·¥ç¨‹å¤æ‚åº¦**: ä¸­ç­‰ã€‚WebSocket å®æ—¶é‡‡é›† + bar èšåˆï¼Œä¸ Step 9 ç»“æ„ç±»ä¼¼ã€‚
>
> **æ•°æ®å¯å¾—æ€§**: Bæ¡£ (éœ€è‡ªå»ºè½ç›˜ï¼Œå¸å®‰æœ‰å®æ—¶æµä½†å†å²æœ‰é™)

#### 12.12.1 æ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Step 10: æ¸…ç®—æ•°æ®é‡‡é›†ä¸å› å­è®¡ç®—                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    1. LiquidationCollector (WebSocket)              â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚   Binance WS â”€â”€â†’ å•ç¬”æ¸…ç®—äº‹ä»¶ â”€â”€â†’ 1m/5m/1h èšåˆ â”€â”€â†’ TimescaleDB    â”‚   â”‚
â”‚  â”‚   !forceOrder@arr   (å®æ—¶æ¨é€)      (bar_close)       (æŒä¹…åŒ–)       â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚   âš ï¸ å¯è§æ€§: bar_close (èšåˆåæ‰å¯ç”¨äºå› å­è®¡ç®—)                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â†“                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    2. æ¸…ç®—å› å­è®¡ç®— (5ä¸ªæ ¸å¿ƒæŒ‡æ ‡)                      â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚   liquidation_volume_long/short, liquidation_imbalance,             â”‚   â”‚
â”‚  â”‚   liquidation_spike, liquidation_momentum                           â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â†“                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    3. æç«¯è¡Œæƒ…é¢„è­¦ä¿¡å·                                â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚   æ¸…ç®—çº§è”æ£€æµ‹ â†’ å¯è§¦å‘é£æ§é™ä»“ / æš‚åœå¼€ä»“                            â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 12.12.2 æ•°æ®æºä¸è·å–æ–¹å¼

**å¸å®‰å¼ºå¹³æ•°æ®æµ (å…è´¹)**:

```python
# WebSocket è®¢é˜…åœ°å€
wss://fstream.binance.com/ws/!forceOrder@arr

# è¿”å›æ•°æ®æ ¼å¼
{
    "e": "forceOrder",                   # äº‹ä»¶ç±»å‹
    "E": 1703001234567,                  # äº‹ä»¶æ—¶é—´
    "o": {
        "s": "BTCUSDT",                  # äº¤æ˜“å¯¹
        "S": "SELL",                     # æ–¹å‘ (SELL=å¤šå¤´è¢«æ¸…ç®—, BUY=ç©ºå¤´è¢«æ¸…ç®—)
        "o": "LIMIT",                    # è®¢å•ç±»å‹
        "f": "IOC",                      # æœ‰æ•ˆæ–¹å¼
        "q": "0.050",                    # æ•°é‡
        "p": "43000.00",                 # ä»·æ ¼
        "ap": "42980.00",                # å¹³å‡æˆäº¤ä»·
        "X": "FILLED",                   # è®¢å•çŠ¶æ€
        "l": "0.050",                    # æœ€æ–°æˆäº¤é‡
        "z": "0.050",                    # ç´¯è®¡æˆäº¤é‡
        "T": 1703001234560               # æˆäº¤æ—¶é—´
    }
}
```

**æ•°æ®å¯å¾—æ€§**:

| é¡¹ç›® | è¯´æ˜ |
|------|------|
| **è´¹ç”¨** | å…è´¹ï¼Œæ— éœ€ API Key |
| **å»¶è¿Ÿ** | å®æ—¶æ¨é€ (<100ms) |
| **å†å²æ•°æ®** | âŒ æ— å†å² APIï¼Œå¿…é¡»è‡ªå»ºè½ç›˜ (Bæ¡£) |
| **æ•°æ®é‡** | å¹³é™æœŸ: ~100æ¡/å°æ—¶ï¼Œæç«¯è¡Œæƒ…: ~10000æ¡/å°æ—¶ |

#### 12.12.3 æ–‡ä»¶ç»“æ„

```
algvex/core/data/collectors/
â”œâ”€â”€ liquidation.py              # LiquidationCollector (æ–°å¢)
â”‚
algvex/core/data/features/
â”œâ”€â”€ liquidation_features.py     # 5ä¸ªæ¸…ç®—å› å­è®¡ç®— (æ–°å¢)
â”‚
algvex/core/risk/
â”œâ”€â”€ liquidation_cascade.py      # æ¸…ç®—çº§è”æ£€æµ‹ (æ–°å¢)
â”‚
tests/p0/
â”œâ”€â”€ test_liquidation_collector.py
â”œâ”€â”€ test_liquidation_features.py
```

#### 12.12.4 LiquidationCollector å®ç°

**æ–‡ä»¶ä½ç½®**: `algvex/core/data/collectors/liquidation.py`

```python
# algvex/core/data/collectors/liquidation.py

import asyncio
import json
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Dict, List, Optional, Callable
from collections import defaultdict
import numpy as np
import pandas as pd
import websockets

from .base import BaseCollector


@dataclass
class LiquidationEvent:
    """å•ç¬”æ¸…ç®—äº‹ä»¶"""
    timestamp: datetime
    symbol: str
    side: str                    # "LONG" æˆ– "SHORT" (è¢«æ¸…ç®—æ–¹å‘)
    quantity: float              # æ¸…ç®—æ•°é‡
    price: float                 # æ¸…ç®—ä»·æ ¼
    notional_usd: float          # æ¸…ç®—é‡‘é¢ (USD)


@dataclass
class AggregatedLiquidationBar:
    """èšåˆåçš„æ¸…ç®— Bar"""
    bar_time: datetime
    symbol: str

    # æ¸…ç®—é‡ç»Ÿè®¡
    volume_long: float           # å¤šå¤´æ¸…ç®—é‡‘é¢ (USD)
    volume_short: float          # ç©ºå¤´æ¸…ç®—é‡‘é¢ (USD)
    count_long: int              # å¤šå¤´æ¸…ç®—ç¬”æ•°
    count_short: int             # ç©ºå¤´æ¸…ç®—ç¬”æ•°

    # æ´¾ç”ŸæŒ‡æ ‡
    total_volume: float          # æ€»æ¸…ç®—é‡‘é¢
    imbalance: float             # å¤šç©ºä¸å¹³è¡¡ (-1 to 1)
    avg_size: float              # å¹³å‡å•ç¬”æ¸…ç®—é‡‘é¢

    # æç«¯è¡Œæƒ…æ ‡è®°
    is_spike: bool               # æ˜¯å¦ä¸ºæ¸…ç®—æ¿€å¢
    spike_ratio: float           # ç›¸å¯¹äºåŸºå‡†çš„å€æ•°

    # å…ƒæ•°æ®
    event_count: int             # bar å†…æ¸…ç®—äº‹ä»¶æ•°
    visibility: str = "bar_close"


class LiquidationCollector(BaseCollector):
    """
    å¸å®‰å¼ºå¹³æ•°æ®é‡‡é›†å™¨ (WebSocket)

    âš ï¸ å¯è§æ€§è§„åˆ™: bar_close
    - æ¸…ç®—æ•°æ®åœ¨ bar ç»“æŸåæ‰èƒ½ç”¨äºå› å­è®¡ç®—
    - é˜²æ­¢æœªæ¥ä¿¡æ¯æ³„éœ²

    âš ï¸ æ³¨æ„: 0 å€¼æ˜¯æ­£å¸¸çš„ (æ²¡æœ‰æ¸…ç®—å‘ç”Ÿ)ï¼Œä¸æ˜¯ç¼ºå¤±
    """

    # å¸å®‰ WebSocket é…ç½®
    WS_URL = "wss://fstream.binance.com/ws/!forceOrder@arr"

    # èšåˆé…ç½®
    BAR_FREQUENCIES = ["1m", "5m", "1h"]

    # æ¸…ç®—æ¿€å¢é˜ˆå€¼
    SPIKE_THRESHOLD = 3.0  # è¶…è¿‡24hå‡å€¼çš„3å€è§†ä¸º spike

    def __init__(self,
                 symbols: Optional[List[str]] = None,
                 bar_freq: str = "1h",
                 on_bar_complete: Optional[Callable] = None):
        """
        Args:
            symbols: è¦è¿½è¸ªçš„äº¤æ˜“å¯¹åˆ—è¡¨ (None=å…¨éƒ¨)
            bar_freq: èšåˆé¢‘ç‡ ("1m", "5m", "1h")
            on_bar_complete: bar å®Œæˆæ—¶çš„å›è°ƒå‡½æ•°
        """
        self.symbols = [s.upper() for s in symbols] if symbols else None
        self.bar_freq = bar_freq
        self.on_bar_complete = on_bar_complete

        # å†…å­˜ç¼“å†²åŒº: symbol -> å½“å‰ bar çš„äº‹ä»¶åˆ—è¡¨
        self._buffers: Dict[str, List[LiquidationEvent]] = defaultdict(list)

        # å½“å‰ bar å¼€å§‹æ—¶é—´
        self._current_bar_start: Dict[str, datetime] = {}

        # 24h æ»šåŠ¨å‡å€¼ (ç”¨äºè®¡ç®— spike)
        self._rolling_avg: Dict[str, float] = defaultdict(lambda: 0.0)
        self._rolling_count: Dict[str, int] = defaultdict(int)

        # è¿è¡ŒçŠ¶æ€
        self._running = False
        self._ws = None

    async def start(self):
        """å¯åŠ¨ WebSocket è¿æ¥å’Œæ•°æ®é‡‡é›†"""
        self._running = True

        while self._running:
            try:
                async with websockets.connect(self.WS_URL) as ws:
                    self._ws = ws
                    await self._receive_loop()
            except Exception as e:
                if self._running:
                    print(f"WebSocket disconnected: {e}, reconnecting in 5s...")
                    await asyncio.sleep(5)

    async def _receive_loop(self):
        """æ¥æ”¶å¹¶å¤„ç†æ¸…ç®—äº‹ä»¶"""
        async for message in self._ws:
            data = json.loads(message)

            # è§£ææ¸…ç®—äº‹ä»¶
            order = data.get("o", {})
            symbol = order.get("s", "")

            # è¿‡æ»¤äº¤æ˜“å¯¹
            if self.symbols and symbol not in self.symbols:
                continue

            # è§£ææ–¹å‘: SELL = å¤šå¤´è¢«æ¸…ç®—, BUY = ç©ºå¤´è¢«æ¸…ç®—
            side = "LONG" if order.get("S") == "SELL" else "SHORT"

            quantity = float(order.get("q", 0))
            price = float(order.get("ap", 0))  # ä½¿ç”¨å¹³å‡æˆäº¤ä»·
            notional = quantity * price

            event = LiquidationEvent(
                timestamp=datetime.fromtimestamp(
                    data.get("E", 0) / 1000, tz=timezone.utc
                ),
                symbol=symbol,
                side=side,
                quantity=quantity,
                price=price,
                notional_usd=notional,
            )

            # æ·»åŠ åˆ°ç¼“å†²åŒº
            self._add_to_buffer(event)

    def _add_to_buffer(self, event: LiquidationEvent):
        """æ·»åŠ äº‹ä»¶åˆ°ç¼“å†²åŒºï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦èšåˆ"""
        symbol = event.symbol

        # è®¡ç®—å½“å‰ bar å¼€å§‹æ—¶é—´
        bar_start = self._get_bar_start(event.timestamp)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å®Œæˆä¸Šä¸€ä¸ª bar
        if symbol in self._current_bar_start:
            if bar_start > self._current_bar_start[symbol]:
                # å®Œæˆä¸Šä¸€ä¸ª bar
                self._complete_bar(symbol, self._current_bar_start[symbol])
                self._buffers[symbol] = []

        self._current_bar_start[symbol] = bar_start
        self._buffers[symbol].append(event)

    def _get_bar_start(self, ts: datetime) -> datetime:
        """è®¡ç®— bar å¼€å§‹æ—¶é—´"""
        if self.bar_freq == "1m":
            return ts.replace(second=0, microsecond=0)
        elif self.bar_freq == "5m":
            minute = (ts.minute // 5) * 5
            return ts.replace(minute=minute, second=0, microsecond=0)
        elif self.bar_freq == "1h":
            return ts.replace(minute=0, second=0, microsecond=0)
        else:
            raise ValueError(f"Unsupported bar_freq: {self.bar_freq}")

    def _complete_bar(self, symbol: str, bar_time: datetime):
        """èšåˆå¹¶è¾“å‡ºä¸€ä¸ªå®Œæ•´çš„ bar"""
        events = self._buffers[symbol]

        # è®¡ç®—èšåˆæŒ‡æ ‡
        aggregated = self._aggregate_events(symbol, bar_time, events)

        # æ›´æ–°æ»šåŠ¨å‡å€¼
        self._update_rolling_avg(symbol, aggregated.total_volume)

        # å›è°ƒ
        if self.on_bar_complete:
            self.on_bar_complete(aggregated)

    def _aggregate_events(self,
                          symbol: str,
                          bar_time: datetime,
                          events: List[LiquidationEvent]) -> AggregatedLiquidationBar:
        """èšåˆæ¸…ç®—äº‹ä»¶ä¸º bar æ•°æ®"""

        volume_long = sum(e.notional_usd for e in events if e.side == "LONG")
        volume_short = sum(e.notional_usd for e in events if e.side == "SHORT")
        count_long = sum(1 for e in events if e.side == "LONG")
        count_short = sum(1 for e in events if e.side == "SHORT")

        total_volume = volume_long + volume_short
        total_count = count_long + count_short

        # è®¡ç®—ä¸å¹³è¡¡åº¦
        if total_volume > 0:
            imbalance = (volume_long - volume_short) / total_volume
        else:
            imbalance = 0.0

        # è®¡ç®—å¹³å‡å•ç¬”å¤§å°
        avg_size = total_volume / total_count if total_count > 0 else 0.0

        # åˆ¤æ–­æ˜¯å¦ä¸º spike
        rolling_avg = self._rolling_avg.get(symbol, 0)
        if rolling_avg > 0:
            spike_ratio = total_volume / rolling_avg
            is_spike = spike_ratio >= self.SPIKE_THRESHOLD
        else:
            spike_ratio = 0.0
            is_spike = False

        return AggregatedLiquidationBar(
            bar_time=bar_time,
            symbol=symbol,
            volume_long=volume_long,
            volume_short=volume_short,
            count_long=count_long,
            count_short=count_short,
            total_volume=total_volume,
            imbalance=imbalance,
            avg_size=avg_size,
            is_spike=is_spike,
            spike_ratio=spike_ratio,
            event_count=len(events),
        )

    def _update_rolling_avg(self, symbol: str, new_volume: float):
        """æ›´æ–°24hæ»šåŠ¨å‡å€¼ (ç®€åŒ–ç‰ˆ: æŒ‡æ•°ç§»åŠ¨å¹³å‡)"""
        alpha = 0.01  # å¹³æ»‘ç³»æ•°
        current = self._rolling_avg.get(symbol, new_volume)
        self._rolling_avg[symbol] = alpha * new_volume + (1 - alpha) * current

    async def stop(self):
        """åœæ­¢é‡‡é›†"""
        self._running = False
        if self._ws:
            await self._ws.close()


# ============== TimescaleDB å­˜å‚¨ ==============

LIQUIDATION_TABLE_SCHEMA = """
CREATE TABLE IF NOT EXISTS liquidation_bars (
    bar_time TIMESTAMPTZ NOT NULL,
    symbol VARCHAR(20) NOT NULL,

    -- æ¸…ç®—é‡
    volume_long DOUBLE PRECISION,
    volume_short DOUBLE PRECISION,
    count_long INTEGER,
    count_short INTEGER,

    -- æ´¾ç”ŸæŒ‡æ ‡
    total_volume DOUBLE PRECISION,
    imbalance DOUBLE PRECISION,
    avg_size DOUBLE PRECISION,

    -- æç«¯è¡Œæƒ…æ ‡è®°
    is_spike BOOLEAN,
    spike_ratio DOUBLE PRECISION,

    -- å…ƒæ•°æ®
    event_count INTEGER,

    PRIMARY KEY (bar_time, symbol)
);

-- åˆ›å»º hypertable
SELECT create_hypertable('liquidation_bars', 'bar_time', if_not_exists => TRUE);

-- ç´¢å¼•
CREATE INDEX IF NOT EXISTS idx_liquidation_symbol ON liquidation_bars (symbol, bar_time DESC);
CREATE INDEX IF NOT EXISTS idx_liquidation_spike ON liquidation_bars (is_spike, bar_time DESC);
"""
```

#### 12.12.5 æ¸…ç®—å› å­è®¡ç®—

**æ–‡ä»¶ä½ç½®**: `algvex/core/data/features/liquidation_features.py`

```python
# algvex/core/data/features/liquidation_features.py

import pandas as pd
import numpy as np
from typing import Dict


class LiquidationFeatureCalculator:
    """
    æ¸…ç®—å› å­è®¡ç®—å™¨

    æ‰€æœ‰å› å­çš„å¯è§æ€§: bar_close
    """

    def calculate_features(self,
                          df: pd.DataFrame,
                          lookback_hours: int = 24) -> pd.DataFrame:
        """
        è®¡ç®—æ¸…ç®—å› å­

        Args:
            df: æ¸…ç®— bar æ•°æ® (ä» TimescaleDB æŸ¥è¯¢)
            lookback_hours: æ»šåŠ¨çª—å£å°æ—¶æ•°

        Returns:
            åŒ…å«æ¸…ç®—å› å­çš„ DataFrame
        """
        features = pd.DataFrame(index=df.index)

        # 1. æ¸…ç®—é‡ (å½’ä¸€åŒ–åˆ°æ—¥å‡å€¼)
        features['liquidation_volume_long'] = self._normalize_volume(
            df['volume_long'], lookback_hours
        )
        features['liquidation_volume_short'] = self._normalize_volume(
            df['volume_short'], lookback_hours
        )

        # 2. æ¸…ç®—ä¸å¹³è¡¡åº¦ (-1 to 1)
        features['liquidation_imbalance'] = df['imbalance']

        # 3. æ¸…ç®—æ¿€å¢æŒ‡æ ‡ (spike detection)
        features['liquidation_spike'] = df['spike_ratio'].clip(0, 10)  # ä¸Šé™10å€

        # 4. æ¸…ç®—åŠ¨é‡ (volume å˜åŒ–è¶‹åŠ¿)
        features['liquidation_momentum'] = self._calculate_momentum(
            df['total_volume'], lookback_hours
        )

        return features

    def _normalize_volume(self,
                         series: pd.Series,
                         lookback_hours: int) -> pd.Series:
        """å½’ä¸€åŒ–æ¸…ç®—é‡ (ç›¸å¯¹äºæ»šåŠ¨å‡å€¼)"""
        # å‡è®¾ 1h bar
        rolling_mean = series.rolling(window=lookback_hours, min_periods=1).mean()
        normalized = series / (rolling_mean + 1e-8)  # é¿å…é™¤é›¶
        return normalized.clip(0, 10)  # ä¸Šé™10å€

    def _calculate_momentum(self,
                           series: pd.Series,
                           lookback_hours: int) -> pd.Series:
        """è®¡ç®—æ¸…ç®—åŠ¨é‡ (çŸ­æœŸ vs é•¿æœŸ)"""
        short_window = max(1, lookback_hours // 6)  # 4h
        long_window = lookback_hours  # 24h

        short_ma = series.rolling(window=short_window, min_periods=1).mean()
        long_ma = series.rolling(window=long_window, min_periods=1).mean()

        momentum = (short_ma - long_ma) / (long_ma + 1e-8)
        return momentum.clip(-5, 5)  # é™åˆ¶èŒƒå›´


# ============== å¯è§æ€§é…ç½® ==============

LIQUIDATION_VISIBILITY = {
    "liquidation_volume_long": "bar_close",
    "liquidation_volume_short": "bar_close",
    "liquidation_imbalance": "bar_close",
    "liquidation_spike": "bar_close",
    "liquidation_momentum": "bar_close",
}
```

#### 12.12.6 æ¸…ç®—çº§è”æ£€æµ‹ (é£æ§é›†æˆ)

**æ–‡ä»¶ä½ç½®**: `algvex/core/risk/liquidation_cascade.py`

```python
# algvex/core/risk/liquidation_cascade.py

from dataclasses import dataclass
from typing import Optional
from datetime import datetime, timedelta
import pandas as pd


@dataclass
class CascadeAlert:
    """æ¸…ç®—çº§è”å‘Šè­¦"""
    timestamp: datetime
    symbol: str
    severity: str              # "warning" / "critical"
    spike_ratio: float
    imbalance: float
    recommendation: str        # "reduce_position" / "pause_new_orders"


class LiquidationCascadeDetector:
    """
    æ¸…ç®—çº§è”æ£€æµ‹å™¨

    ç”¨é€”:
    - æ£€æµ‹æç«¯è¡Œæƒ…é£é™©
    - è§¦å‘é£æ§é™ä»“ / æš‚åœå¼€ä»“
    """

    # å‘Šè­¦é˜ˆå€¼
    WARNING_SPIKE_RATIO = 3.0    # 3å€å‡å€¼
    CRITICAL_SPIKE_RATIO = 5.0   # 5å€å‡å€¼

    # è¿ç»­ spike æ£€æµ‹
    CONSECUTIVE_THRESHOLD = 3    # è¿ç»­3ä¸ªbaréƒ½æ˜¯spike

    def __init__(self, data_manager):
        self.data_manager = data_manager
        self._recent_spikes: dict = {}  # symbol -> spike count

    def check(self, symbol: str) -> Optional[CascadeAlert]:
        """
        æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ¸…ç®—çº§è”é£é™©

        Returns:
            CascadeAlert if risk detected, None otherwise
        """
        # è·å–æœ€è¿‘çš„æ¸…ç®—æ•°æ®
        recent_bars = self.data_manager.get_liquidation_bars(
            symbol=symbol,
            lookback="3h",
        )

        if recent_bars.empty:
            return None

        latest = recent_bars.iloc[-1]

        # æ£€æŸ¥ spike
        if latest['is_spike']:
            self._recent_spikes[symbol] = self._recent_spikes.get(symbol, 0) + 1
        else:
            self._recent_spikes[symbol] = 0

        # åˆ¤æ–­å‘Šè­¦çº§åˆ«
        spike_ratio = latest['spike_ratio']
        consecutive_count = self._recent_spikes.get(symbol, 0)

        if spike_ratio >= self.CRITICAL_SPIKE_RATIO or consecutive_count >= self.CONSECUTIVE_THRESHOLD:
            return CascadeAlert(
                timestamp=datetime.now(),
                symbol=symbol,
                severity="critical",
                spike_ratio=spike_ratio,
                imbalance=latest['imbalance'],
                recommendation="pause_new_orders",
            )
        elif spike_ratio >= self.WARNING_SPIKE_RATIO:
            return CascadeAlert(
                timestamp=datetime.now(),
                symbol=symbol,
                severity="warning",
                spike_ratio=spike_ratio,
                imbalance=latest['imbalance'],
                recommendation="reduce_position",
            )

        return None


# ============== ä¸ RiskManager é›†æˆ ==============

class RiskManagerWithLiquidation:
    """æ‰©å±• RiskManager ä»¥æ”¯æŒæ¸…ç®—çº§è”æ£€æµ‹"""

    def __init__(self, base_risk_manager, cascade_detector):
        self.base = base_risk_manager
        self.cascade_detector = cascade_detector

    def check_order(self, order) -> bool:
        """æ£€æŸ¥è®¢å•æ˜¯å¦å…è®¸æ‰§è¡Œ"""

        # 1. åŸºç¡€é£æ§æ£€æŸ¥
        if not self.base.check_order(order):
            return False

        # 2. æ¸…ç®—çº§è”æ£€æŸ¥
        alert = self.cascade_detector.check(order.symbol)
        if alert:
            if alert.severity == "critical":
                # æš‚åœæ‰€æœ‰æ–°è®¢å•
                return False
            elif alert.severity == "warning":
                # åªå…è®¸å‡ä»“è®¢å•
                if order.is_reduce_only:
                    return True
                return False

        return True
```

#### 12.12.7 æµ‹è¯•ç”¨ä¾‹

```python
# tests/p0/test_liquidation_collector.py

import pytest
from datetime import datetime, timezone
from algvex.core.data.collectors.liquidation import (
    LiquidationCollector, LiquidationEvent, AggregatedLiquidationBar
)


class TestLiquidationEvent:
    """æ¸…ç®—äº‹ä»¶æµ‹è¯•"""

    def test_parse_long_liquidation(self):
        """å¤šå¤´æ¸…ç®—è§£æ"""
        raw = {
            "e": "forceOrder",
            "E": 1703001234567,
            "o": {
                "s": "BTCUSDT",
                "S": "SELL",  # å–å‡º = å¤šå¤´è¢«æ¸…ç®—
                "q": "0.1",
                "ap": "43000.00",
            }
        }
        # è§£æå side åº”ä¸º "LONG"
        # notional = 0.1 * 43000 = 4300 USD

    def test_parse_short_liquidation(self):
        """ç©ºå¤´æ¸…ç®—è§£æ"""
        raw = {
            "o": {
                "s": "BTCUSDT",
                "S": "BUY",  # ä¹°å…¥ = ç©ºå¤´è¢«æ¸…ç®—
                "q": "0.2",
                "ap": "43500.00",
            }
        }
        # è§£æå side åº”ä¸º "SHORT"


class TestAggregation:
    """èšåˆé€»è¾‘æµ‹è¯•"""

    def test_imbalance_calculation(self):
        """ä¸å¹³è¡¡åº¦è®¡ç®—"""
        events = [
            LiquidationEvent(..., side="LONG", notional_usd=100000),
            LiquidationEvent(..., side="SHORT", notional_usd=50000),
        ]
        # imbalance = (100000 - 50000) / 150000 = 0.333

    def test_spike_detection(self):
        """æ¸…ç®—æ¿€å¢æ£€æµ‹"""
        # å½“ volume è¶…è¿‡ 24h å‡å€¼çš„ 3 å€æ—¶ï¼Œis_spike = True


class TestCascadeDetector:
    """æ¸…ç®—çº§è”æ£€æµ‹æµ‹è¯•"""

    def test_warning_alert(self):
        """è­¦å‘Šçº§åˆ«å‘Šè­¦"""
        # spike_ratio >= 3.0 æ—¶è§¦å‘ warning

    def test_critical_alert(self):
        """ä¸¥é‡çº§åˆ«å‘Šè­¦"""
        # spike_ratio >= 5.0 æˆ–è¿ç»­ 3 ä¸ª spike æ—¶è§¦å‘ critical
```

#### 12.12.8 éªŒæ”¶æ ‡å‡†

| éªŒæ”¶é¡¹ | æè¿° | æµ‹è¯•æ–¹æ³• | çŠ¶æ€ |
|--------|------|----------|------|
| LiquidationCollector | WebSocket è¿æ¥ç¨³å®šï¼Œèƒ½é‡‡é›†æ¸…ç®—äº‹ä»¶ | è¿è¡Œ 24h éªŒè¯ | â¬œ |
| äº‹ä»¶è§£æ | æ­£ç¡®åŒºåˆ†å¤šå¤´/ç©ºå¤´æ¸…ç®— | å•å…ƒæµ‹è¯• | â¬œ |
| Bar èšåˆ | 1m/5m/1h èšåˆé€»è¾‘æ­£ç¡® | å•å…ƒæµ‹è¯• | â¬œ |
| 5ä¸ªæ¸…ç®—å› å­ | æ‰€æœ‰å› å­è®¡ç®—æ­£ç¡® | å•å…ƒæµ‹è¯• | â¬œ |
| Spike æ£€æµ‹ | æ¸…ç®—æ¿€å¢æ­£ç¡®æ ‡è®° | å›æ”¾æµ‹è¯• | â¬œ |
| çº§è”æ£€æµ‹ | è¿ç»­ spike æ­£ç¡®è§¦å‘ critical | æ¨¡æ‹Ÿæµ‹è¯• | â¬œ |
| é£æ§é›†æˆ | RiskManager æ­£ç¡®å“åº”å‘Šè­¦ | é›†æˆæµ‹è¯• | â¬œ |
| å¯è§æ€§ | bar_close è§„åˆ™æ— æ³„éœ² | æ³„éœ²æ£€æµ‹æµ‹è¯• | â¬œ |
| TimescaleDB | æ•°æ®æ­£ç¡®å†™å…¥å’ŒæŸ¥è¯¢ | å‹åŠ›æµ‹è¯• | â¬œ |
| é›¶å€¼å¤„ç† | æ— æ¸…ç®—æ—¶æ­£ç¡®è®°å½• 0 è€Œé NULL | å•å…ƒæµ‹è¯• | â¬œ |

---

### 12.13 Step 11: å¤šäº¤æ˜“æ‰€ Basis/ä»·å·®çŸ©é˜µ

> **å¢é‡ä»·å€¼**: å•ä¸€äº¤æ˜“æ‰€ basis å®¹æ˜“è¢«å±€éƒ¨æµåŠ¨æ€§æ‰­æ›²ï¼Œå¤šäº¤æ˜“æ‰€èƒ½æ£€æµ‹"ç»“æ„æ€§åç¦»"ä¸"å¥—åˆ©å‹åŠ›"ã€‚
>
> **å·¥ç¨‹å¤æ‚åº¦**: ä½ã€‚REST API è½®è¯¢ï¼Œæ— éœ€ WebSocketã€‚
>
> **æ•°æ®å¯å¾—æ€§**: Cæ¡£ (éœ€è‡ªè¡Œè®¡ç®—å’Œè½ç›˜)

#### 12.13.1 æ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Step 11: å¤šäº¤æ˜“æ‰€ Basis/ä»·å·®çŸ©é˜µ                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    1. MultiExchangeCollector (REST)                 â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚   Binance â”€â”                                                        â”‚   â”‚
â”‚  â”‚   Bybit   â”€â”¼â”€â”€â†’ ä»·æ ¼å¯¹é½ (asof) â”€â”€â†’ Basisè®¡ç®— â”€â”€â†’ TimescaleDB      â”‚   â”‚
â”‚  â”‚   OKX     â”€â”˜      (UTCç»Ÿä¸€)          (bar_close)    (æŒä¹…åŒ–)        â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚   âš ï¸ å¯è§æ€§: bar_close                                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â†“                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    2. Basis å› å­è®¡ç®— (8ä¸ªæ ¸å¿ƒæŒ‡æ ‡)                    â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚   basis_binance/bybit/okx, basis_consensus, basis_dispersion,       â”‚   â”‚
â”‚  â”‚   cross_exchange_spread, price_discovery_leader, arbitrage_pressure â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â†“                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    3. å¥—åˆ©å‹åŠ›ä¿¡å·                                    â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚   è·¨æ‰€ä»·å·®å¼‚å¸¸ â†’ å¯èƒ½é¢„ç¤ºå¤§é¢èµ„é‡‘æµåŠ¨ / ä»·æ ¼ç»“æ„è°ƒæ•´                   â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 12.13.2 æ•°æ®æºä¸è·å–æ–¹å¼

**ä¸‰ä¸ªäº¤æ˜“æ‰€çš„ä»·æ ¼ API (å…¨éƒ¨å…è´¹)**:

| äº¤æ˜“æ‰€ | ç°è´§ API | æ°¸ç»­ API | é¢‘ç‡é™åˆ¶ |
|--------|----------|----------|----------|
| **Binance** | `GET /api/v3/ticker/price` | `GET /fapi/v1/ticker/price` | 1200/min |
| **Bybit** | `GET /v5/market/tickers?category=spot` | `GET /v5/market/tickers?category=linear` | 600/min |
| **OKX** | `GET /api/v5/market/ticker?instId=BTC-USDT` | `GET /api/v5/market/ticker?instId=BTC-USDT-SWAP` | 20/2s |

**Symbol æ˜ å°„**:

```python
SYMBOL_MAPPING = {
    "BTCUSDT": {
        "binance_spot": "BTCUSDT",
        "binance_perp": "BTCUSDT",
        "bybit_spot": "BTCUSDT",
        "bybit_perp": "BTCUSDT",
        "okx_spot": "BTC-USDT",
        "okx_perp": "BTC-USDT-SWAP",
    },
    "ETHUSDT": {
        "binance_spot": "ETHUSDT",
        "binance_perp": "ETHUSDT",
        "bybit_spot": "ETHUSDT",
        "bybit_perp": "ETHUSDT",
        "okx_spot": "ETH-USDT",
        "okx_perp": "ETH-USDT-SWAP",
    },
    # ... æ›´å¤šäº¤æ˜“å¯¹
}
```

#### 12.13.3 æ–‡ä»¶ç»“æ„

```
algvex/core/data/collectors/
â”œâ”€â”€ multi_exchange.py           # MultiExchangeCollector (æ–°å¢)
â”‚
algvex/core/data/features/
â”œâ”€â”€ basis_features.py           # 8ä¸ª basis å› å­è®¡ç®— (æ–°å¢)
â”‚
algvex/core/config/
â”œâ”€â”€ exchange_symbols.py         # Symbol æ˜ å°„é…ç½® (æ–°å¢)
â”‚
tests/p0/
â”œâ”€â”€ test_multi_exchange_collector.py
â”œâ”€â”€ test_basis_features.py
```

#### 12.13.4 MultiExchangeCollector å®ç°

**æ–‡ä»¶ä½ç½®**: `algvex/core/data/collectors/multi_exchange.py`

```python
# algvex/core/data/collectors/multi_exchange.py

import asyncio
import aiohttp
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Dict, List, Optional
import pandas as pd

from .base import BaseCollector
from ..config.exchange_symbols import SYMBOL_MAPPING


@dataclass
class ExchangePrice:
    """å•ä¸ªäº¤æ˜“æ‰€çš„ä»·æ ¼å¿«ç…§"""
    timestamp: datetime
    exchange: str            # "binance" / "bybit" / "okx"
    symbol: str              # ç»Ÿä¸€ symbol (å¦‚ "BTCUSDT")
    spot_price: float
    perp_price: float
    basis_bps: float         # (spot - perp) / spot * 10000


@dataclass
class AggregatedBasisBar:
    """èšåˆåçš„ Basis Bar"""
    bar_time: datetime
    symbol: str

    # å„äº¤æ˜“æ‰€ basis (bps)
    basis_binance: float
    basis_bybit: float
    basis_okx: float

    # å…±è¯† basis
    basis_consensus: float   # median
    basis_dispersion: float  # std

    # è·¨æ‰€ä»·å·®
    cross_exchange_spread_spot: float   # max - min (bps)
    cross_exchange_spread_perp: float   # max - min (bps)

    # å¥—åˆ©å‹åŠ›æŒ‡æ ‡
    arbitrage_pressure: float  # ä»·å·®å›å½’é€Ÿåº¦

    # å…ƒæ•°æ®
    sample_count: int
    visibility: str = "bar_close"


class MultiExchangeCollector(BaseCollector):
    """
    å¤šäº¤æ˜“æ‰€ä»·æ ¼é‡‡é›†å™¨ (REST API è½®è¯¢)

    âš ï¸ å¯è§æ€§è§„åˆ™: bar_close
    - Basis æ•°æ®åœ¨ bar ç»“æŸåæ‰èƒ½ç”¨äºå› å­è®¡ç®—

    é‡‡é›†é¢‘ç‡: æ¯åˆ†é’Ÿä¸€æ¬¡ (ç¬¦åˆæ‰€æœ‰äº¤æ˜“æ‰€çš„é¢‘ç‡é™åˆ¶)
    """

    # äº¤æ˜“æ‰€ API é…ç½®
    EXCHANGES = {
        "binance": {
            "spot_url": "https://api.binance.com/api/v3/ticker/price",
            "perp_url": "https://fapi.binance.com/fapi/v1/ticker/price",
        },
        "bybit": {
            "spot_url": "https://api.bybit.com/v5/market/tickers",
            "perp_url": "https://api.bybit.com/v5/market/tickers",
        },
        "okx": {
            "base_url": "https://www.okx.com/api/v5/market/ticker",
        },
    }

    def __init__(self,
                 symbols: List[str],
                 poll_interval: int = 60,
                 bar_freq: str = "1m"):
        """
        Args:
            symbols: ç»Ÿä¸€ symbol åˆ—è¡¨ (å¦‚ ["BTCUSDT", "ETHUSDT"])
            poll_interval: è½®è¯¢é—´éš” (ç§’)
            bar_freq: èšåˆé¢‘ç‡
        """
        self.symbols = symbols
        self.poll_interval = poll_interval
        self.bar_freq = bar_freq

        # å†…å­˜ç¼“å†²åŒº
        self._buffers: Dict[str, List[ExchangePrice]] = {}
        self._current_bar_start: Dict[str, datetime] = {}

        self._running = False
        self._session: Optional[aiohttp.ClientSession] = None

    async def start(self):
        """å¯åŠ¨è½®è¯¢é‡‡é›†"""
        self._running = True
        self._session = aiohttp.ClientSession()

        while self._running:
            try:
                await self._poll_all_exchanges()
                await asyncio.sleep(self.poll_interval)
            except Exception as e:
                print(f"Poll error: {e}")
                await asyncio.sleep(5)

    async def _poll_all_exchanges(self):
        """è½®è¯¢æ‰€æœ‰äº¤æ˜“æ‰€"""
        timestamp = datetime.now(timezone.utc)

        # å¹¶å‘è·å–æ‰€æœ‰äº¤æ˜“æ‰€ä»·æ ¼
        tasks = [
            self._fetch_binance_prices(timestamp),
            self._fetch_bybit_prices(timestamp),
            self._fetch_okx_prices(timestamp),
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # åˆå¹¶ç»“æœ
        for result in results:
            if isinstance(result, list):
                for price in result:
                    self._add_to_buffer(price)

    async def _fetch_binance_prices(self, timestamp: datetime) -> List[ExchangePrice]:
        """è·å–å¸å®‰ä»·æ ¼"""
        prices = []

        try:
            # è·å–ç°è´§ä»·æ ¼
            async with self._session.get(self.EXCHANGES["binance"]["spot_url"]) as resp:
                spot_data = {item["symbol"]: float(item["price"])
                            for item in await resp.json()}

            # è·å–æ°¸ç»­ä»·æ ¼
            async with self._session.get(self.EXCHANGES["binance"]["perp_url"]) as resp:
                perp_data = {item["symbol"]: float(item["price"])
                            for item in await resp.json()}

            # è®¡ç®— basis
            for symbol in self.symbols:
                mapping = SYMBOL_MAPPING.get(symbol, {})
                spot_sym = mapping.get("binance_spot", symbol)
                perp_sym = mapping.get("binance_perp", symbol)

                if spot_sym in spot_data and perp_sym in perp_data:
                    spot = spot_data[spot_sym]
                    perp = perp_data[perp_sym]
                    basis_bps = (spot - perp) / spot * 10000

                    prices.append(ExchangePrice(
                        timestamp=timestamp,
                        exchange="binance",
                        symbol=symbol,
                        spot_price=spot,
                        perp_price=perp,
                        basis_bps=basis_bps,
                    ))

        except Exception as e:
            print(f"Binance fetch error: {e}")

        return prices

    async def _fetch_bybit_prices(self, timestamp: datetime) -> List[ExchangePrice]:
        """è·å– Bybit ä»·æ ¼"""
        prices = []

        try:
            # ç°è´§
            url = f"{self.EXCHANGES['bybit']['spot_url']}?category=spot"
            async with self._session.get(url) as resp:
                data = await resp.json()
                spot_data = {item["symbol"]: float(item["lastPrice"])
                            for item in data.get("result", {}).get("list", [])}

            # æ°¸ç»­
            url = f"{self.EXCHANGES['bybit']['perp_url']}?category=linear"
            async with self._session.get(url) as resp:
                data = await resp.json()
                perp_data = {item["symbol"]: float(item["lastPrice"])
                            for item in data.get("result", {}).get("list", [])}

            for symbol in self.symbols:
                mapping = SYMBOL_MAPPING.get(symbol, {})
                spot_sym = mapping.get("bybit_spot", symbol)
                perp_sym = mapping.get("bybit_perp", symbol)

                if spot_sym in spot_data and perp_sym in perp_data:
                    spot = spot_data[spot_sym]
                    perp = perp_data[perp_sym]
                    basis_bps = (spot - perp) / spot * 10000

                    prices.append(ExchangePrice(
                        timestamp=timestamp,
                        exchange="bybit",
                        symbol=symbol,
                        spot_price=spot,
                        perp_price=perp,
                        basis_bps=basis_bps,
                    ))

        except Exception as e:
            print(f"Bybit fetch error: {e}")

        return prices

    async def _fetch_okx_prices(self, timestamp: datetime) -> List[ExchangePrice]:
        """è·å– OKX ä»·æ ¼"""
        prices = []

        for symbol in self.symbols:
            try:
                mapping = SYMBOL_MAPPING.get(symbol, {})
                spot_sym = mapping.get("okx_spot")
                perp_sym = mapping.get("okx_perp")

                if not spot_sym or not perp_sym:
                    continue

                # ç°è´§
                url = f"{self.EXCHANGES['okx']['base_url']}?instId={spot_sym}"
                async with self._session.get(url) as resp:
                    data = await resp.json()
                    spot = float(data["data"][0]["last"])

                # æ°¸ç»­
                url = f"{self.EXCHANGES['okx']['base_url']}?instId={perp_sym}"
                async with self._session.get(url) as resp:
                    data = await resp.json()
                    perp = float(data["data"][0]["last"])

                basis_bps = (spot - perp) / spot * 10000

                prices.append(ExchangePrice(
                    timestamp=timestamp,
                    exchange="okx",
                    symbol=symbol,
                    spot_price=spot,
                    perp_price=perp,
                    basis_bps=basis_bps,
                ))

            except Exception as e:
                print(f"OKX fetch error for {symbol}: {e}")

        return prices

    def _add_to_buffer(self, price: ExchangePrice):
        """æ·»åŠ åˆ°ç¼“å†²åŒº"""
        key = f"{price.symbol}_{price.exchange}"
        bar_start = self._get_bar_start(price.timestamp)

        if key not in self._buffers:
            self._buffers[key] = []
            self._current_bar_start[key] = bar_start

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å®Œæˆä¸Šä¸€ä¸ª bar (ç®€åŒ–: ç”±å¤–éƒ¨å®šæ—¶å™¨è§¦å‘)
        self._buffers[key].append(price)

    def _get_bar_start(self, ts: datetime) -> datetime:
        """è®¡ç®— bar å¼€å§‹æ—¶é—´"""
        if self.bar_freq == "1m":
            return ts.replace(second=0, microsecond=0)
        elif self.bar_freq == "5m":
            minute = (ts.minute // 5) * 5
            return ts.replace(minute=minute, second=0, microsecond=0)
        else:
            return ts.replace(minute=0, second=0, microsecond=0)

    async def stop(self):
        """åœæ­¢é‡‡é›†"""
        self._running = False
        if self._session:
            await self._session.close()


# ============== TimescaleDB å­˜å‚¨ ==============

BASIS_TABLE_SCHEMA = """
CREATE TABLE IF NOT EXISTS basis_bars (
    bar_time TIMESTAMPTZ NOT NULL,
    symbol VARCHAR(20) NOT NULL,

    -- å„äº¤æ˜“æ‰€ basis (bps)
    basis_binance DOUBLE PRECISION,
    basis_bybit DOUBLE PRECISION,
    basis_okx DOUBLE PRECISION,

    -- å…±è¯† basis
    basis_consensus DOUBLE PRECISION,
    basis_dispersion DOUBLE PRECISION,

    -- è·¨æ‰€ä»·å·®
    cross_exchange_spread_spot DOUBLE PRECISION,
    cross_exchange_spread_perp DOUBLE PRECISION,

    -- å¥—åˆ©å‹åŠ›
    arbitrage_pressure DOUBLE PRECISION,

    -- å…ƒæ•°æ®
    sample_count INTEGER,

    PRIMARY KEY (bar_time, symbol)
);

SELECT create_hypertable('basis_bars', 'bar_time', if_not_exists => TRUE);
CREATE INDEX IF NOT EXISTS idx_basis_symbol ON basis_bars (symbol, bar_time DESC);
"""
```

#### 12.13.5 Basis å› å­è®¡ç®—

**æ–‡ä»¶ä½ç½®**: `algvex/core/data/features/basis_features.py`

```python
# algvex/core/data/features/basis_features.py

import pandas as pd
import numpy as np
from typing import Dict, List


class BasisFeatureCalculator:
    """
    Basis å› å­è®¡ç®—å™¨

    æ‰€æœ‰å› å­çš„å¯è§æ€§: bar_close
    """

    def calculate_features(self,
                          df: pd.DataFrame,
                          lookback_hours: int = 24) -> pd.DataFrame:
        """
        è®¡ç®— Basis å› å­

        Args:
            df: Basis bar æ•°æ®
            lookback_hours: æ»šåŠ¨çª—å£

        Returns:
            åŒ…å« Basis å› å­çš„ DataFrame
        """
        features = pd.DataFrame(index=df.index)

        # 1. å„äº¤æ˜“æ‰€ basis (å½’ä¸€åŒ–)
        features['basis_binance'] = df['basis_binance']
        features['basis_bybit'] = df['basis_bybit']
        features['basis_okx'] = df['basis_okx']

        # 2. å…±è¯† basis (ä¸­ä½æ•°)
        basis_cols = ['basis_binance', 'basis_bybit', 'basis_okx']
        features['basis_consensus'] = df[basis_cols].median(axis=1)

        # 3. Basis åˆ†æ•£åº¦ (æ ‡å‡†å·®)
        features['basis_dispersion'] = df[basis_cols].std(axis=1)

        # 4. è·¨æ‰€ä»·å·®
        features['cross_exchange_spread'] = df['cross_exchange_spread_perp']

        # 5. ä»·æ ¼å‘ç°é¢†å¯¼è€… (å“ªä¸ªäº¤æ˜“æ‰€çš„ basis å˜åŒ–é¢†å…ˆ)
        features['price_discovery_leader'] = self._calculate_price_discovery(
            df, lookback_hours
        )

        # 6. å¥—åˆ©å‹åŠ› (ä»·å·®æ”¶æ•›é€Ÿåº¦)
        features['arbitrage_pressure'] = self._calculate_arbitrage_pressure(
            df['cross_exchange_spread_perp'], lookback_hours
        )

        return features

    def _calculate_price_discovery(self,
                                   df: pd.DataFrame,
                                   lookback_hours: int) -> pd.Series:
        """
        è®¡ç®—ä»·æ ¼å‘ç°é¢†å¯¼è€…

        ä½¿ç”¨å„äº¤æ˜“æ‰€ basis å˜åŒ–çš„é¢†å…ˆæ€§ (ç®€åŒ–ç‰ˆ: å˜åŒ–å¹…åº¦æœ€å¤§çš„)
        """
        basis_changes = pd.DataFrame({
            'binance': df['basis_binance'].diff().abs(),
            'bybit': df['basis_bybit'].diff().abs(),
            'okx': df['basis_okx'].diff().abs(),
        })

        # è¿”å›å˜åŒ–æœ€å¤§çš„äº¤æ˜“æ‰€ (ç¼–ç : binance=1, bybit=2, okx=3)
        leader_map = {'binance': 1, 'bybit': 2, 'okx': 3}
        leader = basis_changes.idxmax(axis=1)
        return leader.map(leader_map).fillna(0)

    def _calculate_arbitrage_pressure(self,
                                      spread: pd.Series,
                                      lookback_hours: int) -> pd.Series:
        """
        è®¡ç®—å¥—åˆ©å‹åŠ›

        ä»·å·®è¶Šå¤§ã€æ”¶æ•›è¶Šæ…¢ â†’ å¥—åˆ©å‹åŠ›è¶Šå¤§
        """
        # è®¡ç®—ä»·å·®çš„è‡ªç›¸å…³è¡°å‡
        spread_ma = spread.rolling(window=lookback_hours, min_periods=1).mean()
        deviation = (spread - spread_ma).abs()

        # å½’ä¸€åŒ–
        normalized = deviation / (spread_ma.abs() + 1e-8)
        return normalized.clip(0, 5)


# ============== å¯è§æ€§é…ç½® ==============

BASIS_VISIBILITY = {
    "basis_binance": "bar_close",
    "basis_bybit": "bar_close",
    "basis_okx": "bar_close",
    "basis_consensus": "bar_close",
    "basis_dispersion": "bar_close",
    "cross_exchange_spread": "bar_close",
    "price_discovery_leader": "bar_close",
    "arbitrage_pressure": "bar_close",
}
```

#### 12.13.6 æµ‹è¯•ç”¨ä¾‹

```python
# tests/p0/test_multi_exchange_collector.py

import pytest
from algvex.core.data.collectors.multi_exchange import (
    MultiExchangeCollector, ExchangePrice
)


class TestSymbolMapping:
    """Symbol æ˜ å°„æµ‹è¯•"""

    def test_binance_mapping(self):
        """å¸å®‰ symbol æ˜ å°„æ­£ç¡®"""
        from algvex.core.config.exchange_symbols import SYMBOL_MAPPING
        assert SYMBOL_MAPPING["BTCUSDT"]["binance_spot"] == "BTCUSDT"
        assert SYMBOL_MAPPING["BTCUSDT"]["binance_perp"] == "BTCUSDT"

    def test_okx_mapping(self):
        """OKX symbol æ˜ å°„æ­£ç¡®"""
        from algvex.core.config.exchange_symbols import SYMBOL_MAPPING
        assert SYMBOL_MAPPING["BTCUSDT"]["okx_spot"] == "BTC-USDT"
        assert SYMBOL_MAPPING["BTCUSDT"]["okx_perp"] == "BTC-USDT-SWAP"


class TestBasisCalculation:
    """Basis è®¡ç®—æµ‹è¯•"""

    def test_basis_positive(self):
        """ç°è´§ > æ°¸ç»­æ—¶ basis ä¸ºæ­£"""
        spot = 43500
        perp = 43400
        basis_bps = (spot - perp) / spot * 10000
        assert basis_bps > 0

    def test_basis_negative(self):
        """ç°è´§ < æ°¸ç»­æ—¶ basis ä¸ºè´Ÿ (contango)"""
        spot = 43400
        perp = 43500
        basis_bps = (spot - perp) / spot * 10000
        assert basis_bps < 0


class TestConsensus:
    """å…±è¯† Basis æµ‹è¯•"""

    def test_median_calculation(self):
        """ä¸­ä½æ•°è®¡ç®—æ­£ç¡®"""
        import numpy as np
        basis_values = [10, 15, 12]  # bps
        consensus = np.median(basis_values)
        assert consensus == 12
```

#### 12.13.7 éªŒæ”¶æ ‡å‡†

| éªŒæ”¶é¡¹ | æè¿° | æµ‹è¯•æ–¹æ³• | çŠ¶æ€ |
|--------|------|----------|------|
| Binance Collector | èƒ½æ­£ç¡®è·å–ç°è´§/æ°¸ç»­ä»·æ ¼ | API æµ‹è¯• | â¬œ |
| Bybit Collector | èƒ½æ­£ç¡®è·å–ç°è´§/æ°¸ç»­ä»·æ ¼ | API æµ‹è¯• | â¬œ |
| OKX Collector | èƒ½æ­£ç¡®è·å–ç°è´§/æ°¸ç»­ä»·æ ¼ | API æµ‹è¯• | â¬œ |
| Symbol æ˜ å°„ | å„äº¤æ˜“æ‰€ symbol æ­£ç¡®æ˜ å°„ | å•å…ƒæµ‹è¯• | â¬œ |
| æ—¶åŒºå¯¹é½ | æ‰€æœ‰ä»·æ ¼ç»Ÿä¸€ä¸º UTC | å•å…ƒæµ‹è¯• | â¬œ |
| Basis è®¡ç®— | å„äº¤æ˜“æ‰€ basis è®¡ç®—æ­£ç¡® | å•å…ƒæµ‹è¯• | â¬œ |
| å…±è¯† Basis | ä¸­ä½æ•°å’Œæ ‡å‡†å·®è®¡ç®—æ­£ç¡® | å•å…ƒæµ‹è¯• | â¬œ |
| è·¨æ‰€ä»·å·® | spot/perp ä»·å·®è®¡ç®—æ­£ç¡® | å•å…ƒæµ‹è¯• | â¬œ |
| 8ä¸ª Basis å› å­ | æ‰€æœ‰å› å­è®¡ç®—æ­£ç¡® | å•å…ƒæµ‹è¯• | â¬œ |
| å¯è§æ€§ | bar_close è§„åˆ™æ— æ³„éœ² | æ³„éœ²æ£€æµ‹æµ‹è¯• | â¬œ |
| é¢‘ç‡é™åˆ¶ | ä¸è¶…è¿‡å„äº¤æ˜“æ‰€ API é™åˆ¶ | å‹åŠ›æµ‹è¯• | â¬œ |
| TimescaleDB | æ•°æ®æ­£ç¡®å†™å…¥å’ŒæŸ¥è¯¢ | é›†æˆæµ‹è¯• | â¬œ |

---

### 12.14 æ•°æ®æ‰©å±•è·¯çº¿å›¾ (P2/P3 åç»­)

> **æ ¸å¿ƒåŸåˆ™**: P1 æ•°æ®æ‰©å±•å·²çº³å…¥ Steps 9-11ã€‚P2/P3 å¾…åŸºç¡€è®¾æ–½ç¨³å®šåå†å®æ–½ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ•°æ®æ‰©å±•çŠ¶æ€ (P1 å·²å®Œæˆè§„åˆ’)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  âœ… ã€P1 å·²å‡çº§ä¸º Steps 9-11ã€‘                                               â”‚
â”‚  â”œâ”€ Step 9: L2æ·±åº¦èšåˆ + æ»‘ç‚¹æ ¡å‡† (Section 12.11)                           â”‚
â”‚  â”œâ”€ Step 10: æ¸…ç®—æ•°æ® + çº§è”æ£€æµ‹ (Section 12.12)                            â”‚
â”‚  â””â”€ Step 11: å¤šäº¤æ˜“æ‰€BasisçŸ©é˜µ (Section 12.13)                              â”‚
â”‚                                                                             â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚                                                                             â”‚
â”‚  â³ ã€P2 ä¸­æœŸæ‰©å±•ã€‘æœ‰ä»·å€¼ä½†å·¥ç¨‹é‡è¾ƒå¤§æˆ–å£å¾„éœ€éªŒè¯                              â”‚
â”‚  â”œâ”€ é“¾ä¸Šæµå‘äº¤æ˜“æ‰€ (ç¨³å®šå¸å‡€æµå…¥, BTC/ETHå¤§é¢è½¬è´¦)                            â”‚
â”‚  â””â”€ æ›´ç»†IVç»“æ„ (ä¸åŒDelta/åˆ°æœŸçš„skew, term structure)                        â”‚
â”‚                                                                             â”‚
â”‚  â¸ï¸ ã€P3 è°¨æ…æ‰©å±•ã€‘å…è´¹æ•°æ®ä¸ç¨³å®šï¼Œå®å¯æ™šåš                                   â”‚
â”‚  â””â”€ ç¤¾åª’/æ–°é—» (Reddit, Twitter, Telegram - å…è´¹APIå—é™ä¸¥é‡)                  â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 12.14.1 P2-1: é“¾ä¸Šæµå‘äº¤æ˜“æ‰€

**å¢é‡ä»·å€¼**: æ¯” DefiLlama çš„ä¾›åº”é‡æ›´æ¥è¿‘"äº¤æ˜“é©±åŠ¨"

**æ•°æ®æ¥æº**: å…¬å¼€é“¾ä¸Šæ•°æ® (éœ€è‡ªå»ºè§£ææˆ–ä½¿ç”¨å…è´¹API)

**å¯è§æ€§åˆ†çº§**: B/Cæ¡£

| æŒ‡æ ‡ | è¯´æ˜ | éš¾åº¦ |
|------|------|------|
| stablecoin_exchange_netflow | ç¨³å®šå¸å‡€æµå…¥äº¤æ˜“æ‰€ | ä¸­ |
| btc_exchange_netflow | BTCå‡€æµå…¥äº¤æ˜“æ‰€ | ä¸­ |
| eth_exchange_netflow | ETHå‡€æµå…¥äº¤æ˜“æ‰€ | ä¸­ |
| whale_transfer_count | å¤§é¢è½¬è´¦æ¬¡æ•° (>$1M) | ä½ |
| whale_transfer_volume | å¤§é¢è½¬è´¦é‡‘é¢ | ä½ |

**éš¾ç‚¹**: åœ°å€æ ‡ç­¾éœ€æ‰‹åŠ¨ç»´æŠ¤

---

#### 12.14.2 P2-2: æ›´ç»†IVç»“æ„

**å¢é‡ä»·å€¼**: å¯¹"è¡Œæƒ…åˆ¶åº¦åˆ‡æ¢/å°¾éƒ¨é£é™©"æ›´æ•æ„Ÿ

**æ•°æ®æ¥æº**: Deribit API (å·²æœ‰)

| æŒ‡æ ‡ | è¯´æ˜ |
|------|------|
| iv_25delta_put/call | 25-delta put/callçš„IV |
| iv_skew_25delta | 25-delta skew |
| iv_butterfly | å‡¸æ€§ (wings vs ATM) |
| iv_term_slope | è¿‘æœˆ vs è¿œæœˆ IVå·® |
| vol_surface_pca_1 | æ³¢åŠ¨ç‡æ›²é¢ç¬¬ä¸€ä¸»æˆåˆ† |

---

#### 12.14.3 P3: ç¤¾åª’/æ–°é—» (è°¨æ…)

**å»ºè®®**: åˆ—ä¸º"å¯é€‰/å®éªŒæ€§Cæ¡£"ï¼ŒPhase 3ä¹‹åå†è€ƒè™‘

---

#### 12.14.4 æ–°å¢æ•°æ®çš„å‡†å…¥æ£€æŸ¥æ¸…å•

> **ä»»ä½•æ–°å¢æ•°æ®ï¼Œéƒ½å¿…é¡»å…ˆå›ç­”ä»¥ä¸‹å››é¡¹**:

1. **å¯è§æ€§** - è¿™æ•°æ®åœ¨Tæ—¶åˆ»"ä»€ä¹ˆæ—¶å€™å¯è§"ï¼Ÿ
2. **å¯å¾—æ€§** - å†å²çª—å£å±äºA/B/Cå“ªä¸€æ¡£ï¼Ÿ
3. **å£å¾„ç¨³å®šæ€§** - äº¤æ˜“æ‰€ä¼šä¸ä¼šæ”¹APIï¼Ÿ
4. **å¢é‡éªŒè¯** - ablation + walk-forward éªŒè¯å¢é‡å­˜åœ¨

---

#### 12.14.5 æ•°æ®æ‰©å±•æ£€æŸ¥æ¸…å•

| ä¼˜å…ˆçº§ | æ•°æ®æº | çŠ¶æ€ | è¯¦ç»†æ–¹æ¡ˆ |
|--------|--------|------|----------|
| **P1** | **L2æ·±åº¦èšåˆ** | **â†’ Step 9** | Section 12.11 |
| **P1** | **æ¸…ç®—æ•°æ®** | **â†’ Step 10** | Section 12.12 |
| **P1** | **å¤šäº¤æ˜“æ‰€Basis** | **â†’ Step 11** | Section 12.13 |
| P2 | é“¾ä¸Šæµå‘äº¤æ˜“æ‰€ | â³ å¾…å®æ–½ | Section 12.14.1 |
| P2 | æ›´ç»†IVç»“æ„ | â³ å¾…å®æ–½ | Section 12.14.2 |
| P3 | ç¤¾åª’/æ–°é—» | â¸ï¸ è°¨æ… | Section 12.14.3 |

---

## æ–‡æ¡£æ€»ç»“

### æ ¸å¿ƒèƒ½åŠ›

1. **ç‰©ç†è¾¹ç•Œéš”ç¦»** - production/ vs research/ ç›®å½•éš”ç¦» + CIå¯¼å…¥æ‰«æé—¨ç¦
2. **DataManagerå”¯ä¸€å…¥å£** - ç¦æ­¢ç›´æ¥è®¿é—®DB/Redisï¼Œä¾èµ–æ³¨å…¥éš”ç¦»è¿æ¥ä¿¡æ¯
3. **Canonical Hashing** - è§„èŒƒåŒ–åºåˆ—åŒ–/æ’åº/æµ®ç‚¹ç²¾åº¦ï¼ŒCIè‡ªåŠ¨æ›´æ–°hash
4. **Replayç¡®å®šæ€§** - TimeProvider/SeededRandom/Decimalï¼Œæ¶ˆé™¤éç¡®å®šæ€§
5. **MVP Scopeé…ç½®å¼€å…³** - mvp_scope.yaml è¿è¡Œæ—¶å¼ºåˆ¶æ£€æŸ¥å› å­/æ•°æ®æºè¾¹ç•Œ
6. **åŒé“¾è·¯æ¶æ„** - ç”Ÿäº§é“¾è·¯ä¸ä¾èµ–Qlibï¼Œç ”ç©¶é“¾è·¯ç‹¬ç«‹
7. **MVPèŒƒå›´æ˜ç¡®** - 1æ—¶é—´æ¡†æ¶ (5m) + 20-50æ ‡çš„ + 11æ ¸å¿ƒå› å­
8. **ç‰ˆæœ¬åŒ–é…ç½®** - visibility.yaml + data_contracts/*.yaml + alignment.yaml
9. **å“ˆå¸Œå®¡è®¡** - æ‰€æœ‰é…ç½®æœ‰ config_version + config_hashï¼Œå¯åŠ¨æ—¶æ ¡éªŒ
10. **Daily Replayå¯¹é½** - å¯éªŒè¯çš„å›æµ‹-å®ç›˜é—­ç¯éªŒè¯
11. **4è½®è¿­ä»£äº¤ä»˜** - Iter-1å¥‘çº¦ â†’ Iter-2å¯¹é½ â†’ Iter-3å¿«ç…§ â†’ Iter-4æ‰§è¡Œå±‚
12. **åŠ¨æ€å› å­é—¨æ§›** - ICåŸºå‡†ç›¸å¯¹åŒ–ï¼Œé€‚é…åŠ å¯†è´§å¸é«˜æ³¢åŠ¨ç‰¹æ€§

### å®Œæ•´ç³»ç»Ÿèƒ½åŠ› (ç ”ç©¶ä¾§)

1. **æ•°æ®åˆ†çº§** - 6å¤§ç±»å…è´¹æ•°æ®æºï¼Œæ˜ç¡®A/B/Cä¸‰æ¡£å†å²å¯å¾—æ€§
2. **å› å­ä¸°å¯Œ** - 201ä¸ªæ°¸ç»­ä¸“ç”¨å› å­ (180æ ¸å¿ƒ + 21 P1æ‰©å±•)ï¼Œ**ä»…ç”¨äºç ”ç©¶**
3. **P1æ‰©å±•å®Œæˆ** - L2æ·±åº¦(8) + æ¸…ç®—(5) + å¤šäº¤æ˜“æ‰€Basis(8) = 21ä¸ªæ–°å› å­
4. **å›æµ‹å¯ä¿¡** - 6é¡¹P0éªŒæ”¶ + Steps 9-11 æ»‘ç‚¹æ ¡å‡†
5. **æ‰§è¡Œå¯é ** - Hummingbot v2.11.0 ä¼ä¸šçº§æˆç†Ÿåº¦
6. **å¯å¤ç°** - æ•°æ®å¿«ç…§ + Trace Schema + å®Œæ•´è¡€ç¼˜é“¾
7. **é›¶æ•°æ®æˆæœ¬** - å…¨éƒ¨ä½¿ç”¨å…è´¹å…¬å¼€æ•°æ®

> **MVP vs å®Œæ•´ç³»ç»Ÿ**: MVPç”Ÿäº§ç®¡é“ä»…ä½¿ç”¨11ä¸ªéªŒè¯å› å­ + 3ä¸ªæ•°æ®æºï¼Œ201å› å­ä½“ç³»ä»…ç”¨äºç ”ç©¶/å›æµ‹ï¼Œä¸è¿›å…¥ç”Ÿäº§ä»£ç ã€‚

---

*æ–‡æ¡£ç‰ˆæœ¬: v2.0.0 | æ›´æ–°äº 2025-12-31*