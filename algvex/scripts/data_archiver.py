#!/usr/bin/env python3
"""
AlgVex æ•°æ®è‡ªåŠ¨è½ç›˜è„šæœ¬

åŠŸèƒ½:
1. å®šæœŸä»å¸å®‰é‡‡é›†æ‰€æœ‰å…è´¹æ•°æ®
2. å¢é‡ä¿å­˜åˆ°æœ¬åœ° Parquet æ–‡ä»¶
3. è‡ªåŠ¨å»é‡å’Œæ•°æ®åˆå¹¶
4. æ”¯æŒå¤šç§è¿è¡Œæ¨¡å¼ (ä¸€æ¬¡æ€§/å®šæ—¶/å®ˆæŠ¤è¿›ç¨‹)

ä½¿ç”¨æ–¹æ³•:
    # ä¸€æ¬¡æ€§é‡‡é›†
    python data_archiver.py --once

    # å®šæ—¶é‡‡é›† (æ¯å°æ—¶)
    python data_archiver.py --interval 3600

    # å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼
    python data_archiver.py --daemon

æ•°æ®å­˜å‚¨ç»“æ„:
    ~/.algvex/data/
    â”œâ”€â”€ klines/
    â”‚   â”œâ”€â”€ BTCUSDT_1h.parquet
    â”‚   â””â”€â”€ ETHUSDT_1h.parquet
    â”œâ”€â”€ funding/
    â”‚   â””â”€â”€ funding_rate.parquet
    â”œâ”€â”€ oi/
    â”‚   â””â”€â”€ open_interest.parquet
    â”œâ”€â”€ ls_ratio/
    â”‚   â””â”€â”€ long_short_ratio.parquet
    â””â”€â”€ taker/
        â””â”€â”€ taker_buy_sell.parquet
"""

import argparse
import os
import sys
import time
import signal
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from algvex.core.data.collector import BinanceDataCollector

try:
    from loguru import logger
    # é…ç½®æ—¥å¿—
    logger.remove()
    logger.add(sys.stderr, level="INFO", format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{message}</cyan>")
except ImportError:
    import logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)-8s | %(message)s')
    logger = logging.getLogger(__name__)


class DataArchiver:
    """
    æ•°æ®è½ç›˜ç®¡ç†å™¨

    ç‰¹æ€§:
    - å¢é‡é‡‡é›†: åªé‡‡é›†æ–°æ•°æ®
    - è‡ªåŠ¨å»é‡: åŸºäºæ—¶é—´æˆ³å»é‡
    - æ•…éšœæ¢å¤: æ–­ç‚¹ç»­é‡‡
    - æ•°æ®å‹ç¼©: Parquetæ ¼å¼é«˜æ•ˆå­˜å‚¨
    """

    # é»˜è®¤é…ç½®
    DEFAULT_SYMBOLS = ["BTCUSDT", "ETHUSDT", "BNBUSDT", "SOLUSDT", "XRPUSDT"]
    DEFAULT_DATA_DIR = "~/.algvex/data"
    DEFAULT_INTERVAL = "1h"

    def __init__(
        self,
        symbols: List[str] = None,
        data_dir: str = None,
        interval: str = None,
        rate_limit_delay: float = 0.1,
    ):
        """
        åˆå§‹åŒ–è½ç›˜ç®¡ç†å™¨

        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
            interval: Kçº¿å‘¨æœŸ
            rate_limit_delay: APIè°ƒç”¨é—´éš”
        """
        self.symbols = symbols or self.DEFAULT_SYMBOLS
        self.data_dir = Path(data_dir or self.DEFAULT_DATA_DIR).expanduser()
        self.interval = interval or self.DEFAULT_INTERVAL
        self.rate_limit_delay = rate_limit_delay

        # åˆ›å»ºç›®å½•ç»“æ„
        self._create_directories()

        # åˆå§‹åŒ–é‡‡é›†å™¨
        self.collector = BinanceDataCollector(
            symbols=self.symbols,
            data_dir=str(self.data_dir),
            rate_limit_delay=rate_limit_delay,
        )

        # è¿è¡ŒçŠ¶æ€
        self._running = True
        self._setup_signal_handlers()

        logger.info(f"DataArchiver initialized")
        logger.info(f"  Symbols: {self.symbols}")
        logger.info(f"  Data dir: {self.data_dir}")
        logger.info(f"  Interval: {self.interval}")

    def _create_directories(self):
        """åˆ›å»ºæ•°æ®ç›®å½•ç»“æ„"""
        dirs = ["klines", "funding", "oi", "ls_ratio", "taker", "logs"]
        for d in dirs:
            (self.data_dir / d).mkdir(parents=True, exist_ok=True)

    def _setup_signal_handlers(self):
        """è®¾ç½®ä¿¡å·å¤„ç†"""
        def handler(signum, frame):
            logger.info("Received shutdown signal, stopping...")
            self._running = False

        signal.signal(signal.SIGINT, handler)
        signal.signal(signal.SIGTERM, handler)

    def _get_last_timestamp(self, data_type: str, symbol: str = None) -> Optional[int]:
        """è·å–æœ€åä¸€æ¡æ•°æ®çš„æ—¶é—´æˆ³"""
        if data_type == "klines":
            file_path = self.data_dir / "klines" / f"{symbol}_{self.interval}.parquet"
        else:
            file_path = self.data_dir / data_type / f"{data_type}.parquet"

        if not file_path.exists():
            return None

        try:
            df = pd.read_parquet(file_path)
            if df.empty:
                return None

            if symbol and "symbol" in df.columns:
                df = df[df["symbol"] == symbol]

            if df.empty:
                return None

            last_time = df["datetime"].max()
            if pd.isna(last_time):
                return None

            return int(pd.Timestamp(last_time).timestamp() * 1000)
        except Exception as e:
            logger.warning(f"Failed to read last timestamp from {file_path}: {e}")
            return None

    def _save_incremental(self, data_type: str, df: pd.DataFrame, symbol: str = None):
        """å¢é‡ä¿å­˜æ•°æ®"""
        if df.empty:
            return

        if data_type == "klines":
            file_path = self.data_dir / "klines" / f"{symbol}_{self.interval}.parquet"
        else:
            file_path = self.data_dir / data_type / f"{data_type}.parquet"

        # è¯»å–ç°æœ‰æ•°æ®
        if file_path.exists():
            try:
                existing_df = pd.read_parquet(file_path)
                # åˆå¹¶å¹¶å»é‡
                combined_df = pd.concat([existing_df, df], ignore_index=True)
                combined_df = combined_df.drop_duplicates(
                    subset=["datetime", "symbol"] if "symbol" in combined_df.columns else ["datetime"],
                    keep="last"
                )
                combined_df = combined_df.sort_values("datetime").reset_index(drop=True)
            except Exception as e:
                logger.warning(f"Failed to read existing data, overwriting: {e}")
                combined_df = df
        else:
            combined_df = df

        # ä¿å­˜
        combined_df.to_parquet(file_path, index=False)
        logger.info(f"Saved {len(df)} new rows to {file_path.name} (total: {len(combined_df)})")

    def collect_klines(self, symbol: str) -> pd.DataFrame:
        """é‡‡é›†Kçº¿æ•°æ® (å¢é‡)"""
        # è·å–ä¸Šæ¬¡é‡‡é›†çš„æœ€åæ—¶é—´
        last_ts = self._get_last_timestamp("klines", symbol)

        if last_ts:
            # ä»ä¸Šæ¬¡ç»“æŸæ—¶é—´å¼€å§‹é‡‡é›†
            start_ts = last_ts + 1
            logger.debug(f"Incremental klines for {symbol} from {datetime.fromtimestamp(start_ts/1000)}")
        else:
            # é¦–æ¬¡é‡‡é›†ï¼Œè·å–æœ€è¿‘30å¤©
            start_ts = int((datetime.now() - timedelta(days=30)).timestamp() * 1000)
            logger.debug(f"Initial klines for {symbol}")

        end_ts = int(datetime.now().timestamp() * 1000)

        # åˆ†é¡µé‡‡é›†
        all_data = []
        current_start = start_ts

        while current_start < end_ts:
            df = self.collector.fetch_klines(
                symbol=symbol,
                interval=self.interval,
                start_time=current_start,
                end_time=end_ts,
                limit=1500
            )

            if df.empty:
                break

            all_data.append(df)

            # æ›´æ–°èµ·å§‹æ—¶é—´
            last_time = df["datetime"].max()
            current_start = int(pd.Timestamp(last_time).timestamp() * 1000) + 1

            # å¦‚æœè¿”å›æ•°é‡å°‘äºlimitï¼Œè¯´æ˜å·²ç»åˆ°è¾¾ç»ˆç‚¹
            if len(df) < 1500:
                break

        if all_data:
            result = pd.concat(all_data, ignore_index=True).drop_duplicates(subset=["datetime"])
            return result
        return pd.DataFrame()

    def collect_funding_rate(self, symbol: str) -> pd.DataFrame:
        """é‡‡é›†èµ„é‡‘è´¹ç‡ (å¢é‡)"""
        last_ts = self._get_last_timestamp("funding", symbol)

        if last_ts:
            start_ts = last_ts + 1
        else:
            # èµ„é‡‘è´¹ç‡å†å²è¾ƒé•¿ï¼Œå°è¯•è·å–æ›´å¤š
            start_ts = int((datetime.now() - timedelta(days=365)).timestamp() * 1000)

        end_ts = int(datetime.now().timestamp() * 1000)

        # åˆ†é¡µé‡‡é›†
        all_data = []
        current_start = start_ts

        while current_start < end_ts:
            df = self.collector.fetch_funding_rate(
                symbol=symbol,
                start_time=current_start,
                end_time=end_ts,
                limit=1000
            )

            if df.empty:
                break

            all_data.append(df)

            last_time = df["datetime"].max()
            current_start = int(pd.Timestamp(last_time).timestamp() * 1000) + 1

            if len(df) < 1000:
                break

        if all_data:
            result = pd.concat(all_data, ignore_index=True).drop_duplicates(subset=["datetime", "symbol"])
            return result
        return pd.DataFrame()

    def collect_open_interest(self, symbol: str) -> pd.DataFrame:
        """é‡‡é›†æŒä»“é‡ (ä»…æœ€è¿‘30å¤©å¯ç”¨)"""
        # æŒä»“é‡å†å²APIåªæœ‰çº¦30å¤©æ•°æ®
        df = self.collector.fetch_open_interest_history(
            symbol=symbol,
            period=self.interval,
            limit=500
        )
        return df

    def collect_long_short_ratio(self, symbol: str) -> pd.DataFrame:
        """é‡‡é›†å¤šç©ºæ¯” (ä»…æœ€è¿‘30å¤©å¯ç”¨)"""
        df = self.collector.fetch_long_short_ratio(
            symbol=symbol,
            period=self.interval,
            limit=500
        )
        return df

    def collect_taker_ratio(self, symbol: str) -> pd.DataFrame:
        """é‡‡é›†ä¸»åŠ¨ä¹°å–æ¯” (ä»…æœ€è¿‘30å¤©å¯ç”¨)"""
        df = self.collector.fetch_taker_long_short_ratio(
            symbol=symbol,
            period=self.interval,
            limit=500
        )
        return df

    def run_once(self) -> Dict[str, int]:
        """
        æ‰§è¡Œä¸€æ¬¡å®Œæ•´é‡‡é›†

        Returns:
            å„æ•°æ®ç±»å‹é‡‡é›†çš„è®°å½•æ•°
        """
        logger.info("=" * 50)
        logger.info(f"Starting data collection at {datetime.now()}")
        logger.info("=" * 50)

        stats = {
            "klines": 0,
            "funding": 0,
            "oi": 0,
            "ls_ratio": 0,
            "taker": 0,
        }

        for symbol in self.symbols:
            if not self._running:
                break

            logger.info(f"Collecting data for {symbol}...")

            # 1. Kçº¿ (æœ‰å®Œæ•´å†å²)
            try:
                klines = self.collect_klines(symbol)
                if not klines.empty:
                    self._save_incremental("klines", klines, symbol)
                    stats["klines"] += len(klines)
            except Exception as e:
                logger.error(f"Failed to collect klines for {symbol}: {e}")

            # 2. èµ„é‡‘è´¹ç‡ (æœ‰å®Œæ•´å†å²)
            try:
                funding = self.collect_funding_rate(symbol)
                if not funding.empty:
                    self._save_incremental("funding", funding)
                    stats["funding"] += len(funding)
            except Exception as e:
                logger.error(f"Failed to collect funding rate for {symbol}: {e}")

            # 3. æŒä»“é‡ (ä»…30å¤©)
            try:
                oi = self.collect_open_interest(symbol)
                if not oi.empty:
                    self._save_incremental("oi", oi)
                    stats["oi"] += len(oi)
            except Exception as e:
                logger.error(f"Failed to collect open interest for {symbol}: {e}")

            # 4. å¤šç©ºæ¯” (ä»…30å¤©)
            try:
                ls_ratio = self.collect_long_short_ratio(symbol)
                if not ls_ratio.empty:
                    self._save_incremental("ls_ratio", ls_ratio)
                    stats["ls_ratio"] += len(ls_ratio)
            except Exception as e:
                logger.error(f"Failed to collect long/short ratio for {symbol}: {e}")

            # 5. ä¸»åŠ¨ä¹°å–æ¯” (ä»…30å¤©)
            try:
                taker = self.collect_taker_ratio(symbol)
                if not taker.empty:
                    self._save_incremental("taker", taker)
                    stats["taker"] += len(taker)
            except Exception as e:
                logger.error(f"Failed to collect taker ratio for {symbol}: {e}")

        logger.info("-" * 50)
        logger.info("Collection completed:")
        for k, v in stats.items():
            logger.info(f"  {k}: {v} records")
        logger.info("-" * 50)

        return stats

    def run_scheduled(self, interval_seconds: int = 3600):
        """
        å®šæ—¶è¿è¡Œé‡‡é›†

        Args:
            interval_seconds: é‡‡é›†é—´éš”(ç§’), é»˜è®¤1å°æ—¶
        """
        logger.info(f"Starting scheduled collection (interval: {interval_seconds}s)")

        while self._running:
            try:
                self.run_once()
            except Exception as e:
                logger.error(f"Collection failed: {e}")

            if not self._running:
                break

            # ç­‰å¾…ä¸‹æ¬¡é‡‡é›†
            logger.info(f"Next collection in {interval_seconds} seconds...")
            for _ in range(interval_seconds):
                if not self._running:
                    break
                time.sleep(1)

        logger.info("Scheduled collection stopped")

    def get_data_stats(self) -> Dict[str, Dict]:
        """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}

        # Kçº¿ç»Ÿè®¡
        klines_dir = self.data_dir / "klines"
        if klines_dir.exists():
            klines_stats = {}
            for f in klines_dir.glob("*.parquet"):
                try:
                    df = pd.read_parquet(f)
                    klines_stats[f.stem] = {
                        "rows": len(df),
                        "start": str(df["datetime"].min()) if not df.empty else None,
                        "end": str(df["datetime"].max()) if not df.empty else None,
                    }
                except Exception:
                    pass
            stats["klines"] = klines_stats

        # å…¶ä»–æ•°æ®ç»Ÿè®¡
        for data_type in ["funding", "oi", "ls_ratio", "taker"]:
            file_path = self.data_dir / data_type / f"{data_type}.parquet"
            if file_path.exists():
                try:
                    df = pd.read_parquet(file_path)
                    stats[data_type] = {
                        "rows": len(df),
                        "start": str(df["datetime"].min()) if not df.empty else None,
                        "end": str(df["datetime"].max()) if not df.empty else None,
                        "symbols": list(df["symbol"].unique()) if "symbol" in df.columns else [],
                    }
                except Exception:
                    pass

        return stats


def main():
    parser = argparse.ArgumentParser(
        description="AlgVex æ•°æ®è‡ªåŠ¨è½ç›˜è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä¸€æ¬¡æ€§é‡‡é›†
  python data_archiver.py --once

  # æ¯å°æ—¶é‡‡é›†ä¸€æ¬¡
  python data_archiver.py --interval 3600

  # æŸ¥çœ‹æ•°æ®ç»Ÿè®¡
  python data_archiver.py --stats

  # è‡ªå®šä¹‰äº¤æ˜“å¯¹å’Œç›®å½•
  python data_archiver.py --once --symbols BTCUSDT,ETHUSDT --data-dir /data/crypto
        """
    )

    parser.add_argument(
        "--once",
        action="store_true",
        help="æ‰§è¡Œä¸€æ¬¡é‡‡é›†åé€€å‡º"
    )
    parser.add_argument(
        "--interval",
        type=int,
        default=3600,
        help="å®šæ—¶é‡‡é›†é—´éš”(ç§’), é»˜è®¤3600"
    )
    parser.add_argument(
        "--daemon",
        action="store_true",
        help="å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼è¿è¡Œ"
    )
    parser.add_argument(
        "--stats",
        action="store_true",
        help="æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯"
    )
    parser.add_argument(
        "--symbols",
        type=str,
        default=None,
        help="äº¤æ˜“å¯¹åˆ—è¡¨(é€—å·åˆ†éš”), ä¾‹å¦‚: BTCUSDT,ETHUSDT"
    )
    parser.add_argument(
        "--data-dir",
        type=str,
        default=None,
        help="æ•°æ®å­˜å‚¨ç›®å½•, é»˜è®¤: ~/.algvex/data"
    )
    parser.add_argument(
        "--kline-interval",
        type=str,
        default="1h",
        help="Kçº¿å‘¨æœŸ, é»˜è®¤: 1h"
    )

    args = parser.parse_args()

    # è§£æäº¤æ˜“å¯¹
    symbols = None
    if args.symbols:
        symbols = [s.strip().upper() for s in args.symbols.split(",")]

    # åˆå§‹åŒ–
    archiver = DataArchiver(
        symbols=symbols,
        data_dir=args.data_dir,
        interval=args.kline_interval,
    )

    # æ‰§è¡Œç›¸åº”æ“ä½œ
    if args.stats:
        stats = archiver.get_data_stats()
        print("\nğŸ“Š æ•°æ®ç»Ÿè®¡ä¿¡æ¯:")
        print("=" * 60)
        for data_type, info in stats.items():
            print(f"\nğŸ“ {data_type}:")
            if isinstance(info, dict) and "rows" in info:
                print(f"   è®°å½•æ•°: {info['rows']}")
                print(f"   å¼€å§‹æ—¶é—´: {info['start']}")
                print(f"   ç»“æŸæ—¶é—´: {info['end']}")
                if info.get("symbols"):
                    print(f"   äº¤æ˜“å¯¹: {', '.join(info['symbols'])}")
            elif isinstance(info, dict):
                for name, details in info.items():
                    print(f"   {name}: {details['rows']} æ¡ ({details['start']} ~ {details['end']})")
        print("\n" + "=" * 60)
        return

    if args.once:
        archiver.run_once()
    elif args.daemon or args.interval:
        archiver.run_scheduled(args.interval)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
