"""
åŠ å¯†è´§å¸æ•°æ®å‡†å¤‡è„šæœ¬ (v10.2.0) - ç›´æ¥å®˜æ–¹æ•°æ®æº

ç›´æ¥ä» Binance å®˜æ–¹æ•°æ®ä»“åº“ (data.binance.vision) ä¸‹è½½å†å² K çº¿æ•°æ®ã€‚
æ— ç¬¬ä¸‰æ–¹åŒ…ä¾èµ–ï¼Œä»…ä½¿ç”¨ requests + pandasã€‚

æ•°æ®æº: https://data.binance.vision/
è¾“å‡ºç›®å½•: ~/.algvex/data/{interval}/
è¾“å‡ºæ ¼å¼: Parquet

ç”¨æ³•:
    python scripts/prepare_crypto_data.py --trading-pairs BTC-USDT ETH-USDT --interval 1h

ç¯å¢ƒå˜é‡:
    ALGVEX_DATA_DIR: è‡ªå®šä¹‰æ•°æ®ç›®å½• (é»˜è®¤ ~/.algvex/data)
    HTTPS_PROXY: ä»£ç†æœåŠ¡å™¨ (å¦‚ http://127.0.0.1:7890)
"""

import os
import sys
import io
import json
import zipfile
import argparse
from pathlib import Path
from datetime import date, datetime, timedelta
from typing import List, Dict, Optional, Tuple

import requests
import pandas as pd


# ============================================================================
# å¸¸é‡å®šä¹‰
# ============================================================================

# Binance å®˜æ–¹æ•°æ®æº URL
BASE_URL = "https://data.binance.vision/data/spot"

# æ”¯æŒçš„æ—¶é—´é—´éš”
SUPPORTED_INTERVALS = ["1m", "3m", "5m", "15m", "30m", "1h", "2h", "4h", "6h", "8h", "12h", "1d"]

# CSV åˆ—å (Binance å®˜æ–¹æ ¼å¼)
KLINE_COLUMNS = [
    "open_time", "open", "high", "low", "close", "volume",
    "close_time", "quote_volume", "trades", "taker_buy_base",
    "taker_buy_quote", "ignore"
]


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def trading_pair_to_symbol(pair: str) -> str:
    """BTC-USDT -> BTCUSDT"""
    return pair.replace("-", "").upper()


def get_default_data_dir() -> Path:
    """è·å–é»˜è®¤æ•°æ®ç›®å½•"""
    return Path(os.environ.get("ALGVEX_DATA_DIR", Path.home() / ".algvex" / "data"))


def get_proxy_config() -> Optional[Dict[str, str]]:
    """è·å–ä»£ç†é…ç½®"""
    proxy = os.environ.get("HTTPS_PROXY") or os.environ.get("https_proxy")
    if proxy:
        return {"http": proxy, "https": proxy}
    return None


def generate_month_list(start_date: date, end_date: date) -> List[Tuple[int, int]]:
    """ç”Ÿæˆæœˆä»½åˆ—è¡¨: [(year, month), ...]"""
    months = []
    current = date(start_date.year, start_date.month, 1)
    end = date(end_date.year, end_date.month, 1)

    while current <= end:
        months.append((current.year, current.month))
        # ä¸‹ä¸€ä¸ªæœˆ
        if current.month == 12:
            current = date(current.year + 1, 1, 1)
        else:
            current = date(current.year, current.month + 1, 1)

    return months


# ============================================================================
# æ•°æ®ä¸‹è½½
# ============================================================================

def download_monthly_klines(
    symbol: str,
    interval: str,
    year: int,
    month: int,
    proxies: Optional[Dict[str, str]] = None,
    timeout: int = 60,
) -> Optional[pd.DataFrame]:
    """
    ä¸‹è½½å•æœˆ K çº¿æ•°æ®

    URL æ ¼å¼: https://data.binance.vision/data/spot/monthly/klines/{symbol}/{interval}/{symbol}-{interval}-{year}-{month}.zip
    """
    filename = f"{symbol}-{interval}-{year}-{month:02d}.zip"
    url = f"{BASE_URL}/monthly/klines/{symbol}/{interval}/{filename}"

    try:
        response = requests.get(url, proxies=proxies, timeout=timeout)

        if response.status_code == 200:
            # è§£å‹ ZIP å¹¶è¯»å– CSV
            with zipfile.ZipFile(io.BytesIO(response.content)) as zf:
                csv_name = filename.replace(".zip", ".csv")
                with zf.open(csv_name) as f:
                    df = pd.read_csv(f, header=None, names=KLINE_COLUMNS)
                    return df

        elif response.status_code == 404:
            # æ•°æ®ä¸å­˜åœ¨ (å¯èƒ½æ˜¯æœªæ¥æœˆä»½æˆ–äº¤æ˜“å¯¹ä¸å­˜åœ¨)
            return None
        else:
            print(f"      HTTP {response.status_code}")
            return None

    except requests.exceptions.Timeout:
        print(f"      è¶…æ—¶")
        return None
    except requests.exceptions.ConnectionError as e:
        print(f"      è¿æ¥é”™è¯¯: {e}")
        return None
    except Exception as e:
        print(f"      é”™è¯¯: {e}")
        return None


def download_symbol_data(
    symbol: str,
    interval: str,
    start_date: date,
    end_date: date,
    proxies: Optional[Dict[str, str]] = None,
) -> pd.DataFrame:
    """
    ä¸‹è½½æŒ‡å®šäº¤æ˜“å¯¹çš„å®Œæ•´æ•°æ®
    """
    months = generate_month_list(start_date, end_date)
    all_dfs = []

    print(f"\n   ä¸‹è½½ {symbol} ({len(months)} ä¸ªæœˆä»½)...")

    for i, (year, month) in enumerate(months):
        print(f"      [{i+1}/{len(months)}] {year}-{month:02d}", end=" ")

        df = download_monthly_klines(symbol, interval, year, month, proxies)

        if df is not None and not df.empty:
            all_dfs.append(df)
            print(f"âœ“ {len(df)} è¡Œ")
        else:
            print("- æ— æ•°æ®")

    if not all_dfs:
        return pd.DataFrame()

    # åˆå¹¶æ‰€æœ‰æœˆä»½æ•°æ®
    combined = pd.concat(all_dfs, ignore_index=True)
    combined = combined.drop_duplicates(subset=["open_time"])
    combined = combined.sort_values("open_time")

    return combined


# ============================================================================
# æ•°æ®è½¬æ¢
# ============================================================================

def convert_to_parquet_format(df: pd.DataFrame, start_date: date, end_date: date) -> pd.DataFrame:
    """
    å°† Binance CSV æ ¼å¼è½¬æ¢ä¸º AlgVex Parquet æ ¼å¼

    è¾“å‡º:
    - Index: datetime (UTC)
    - Columns: open, high, low, close, volume, quote_volume
    """
    if df.empty:
        return pd.DataFrame()

    # æ£€æµ‹æ—¶é—´æˆ³å•ä½
    sample_ts = df["open_time"].iloc[0]
    if sample_ts > 1e15:
        unit = "us"  # å¾®ç§’ (2025å¹´èµ·)
    elif sample_ts > 1e12:
        unit = "ms"  # æ¯«ç§’
    else:
        unit = "s"   # ç§’

    # è½¬æ¢æ—¶é—´æˆ³
    df = df.copy()
    df["datetime"] = pd.to_datetime(df["open_time"], unit=unit, utc=True)
    df = df.set_index("datetime")

    # è¿‡æ»¤æ—¶é—´èŒƒå›´
    start_ts = pd.Timestamp(start_date, tz="UTC")
    end_ts = pd.Timestamp(end_date, tz="UTC") + pd.Timedelta(days=1)
    df = df[(df.index >= start_ts) & (df.index < end_ts)]

    # åªä¿ç•™éœ€è¦çš„åˆ—
    result = pd.DataFrame({
        "open": df["open"].astype(float),
        "high": df["high"].astype(float),
        "low": df["low"].astype(float),
        "close": df["close"].astype(float),
        "volume": df["volume"].astype(float),
        "quote_volume": df["quote_volume"].astype(float),
    })

    return result


def save_to_parquet(
    data: Dict[str, pd.DataFrame],
    output_dir: Path,
    interval: str,
) -> None:
    """ä¿å­˜ä¸º Parquet æ ¼å¼"""
    freq_dir = output_dir / interval
    freq_dir.mkdir(parents=True, exist_ok=True)

    metadata = {
        "freq": interval,
        "timezone": "UTC",
        "source": "data.binance.vision",
        "version": "v10.2.0",
        "download_time": datetime.now().isoformat(),
        "instruments": [],
        "columns": ["open", "high", "low", "close", "volume", "quote_volume"],
    }

    for symbol, df in data.items():
        instrument = symbol.lower()
        file_path = freq_dir / f"{instrument}.parquet"

        df.to_parquet(file_path, engine="pyarrow")

        metadata["instruments"].append({
            "name": instrument,
            "symbol": symbol,
            "start": df.index.min().isoformat(),
            "end": df.index.max().isoformat(),
            "rows": len(df),
        })

        print(f"   âœ… {instrument}.parquet: {len(df):,} è¡Œ")

    # ä¿å­˜å…ƒæ•°æ®
    with open(freq_dir / "metadata.json", "w") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“ æ•°æ®ä¿å­˜åˆ°: {freq_dir}")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="ä» Binance å®˜æ–¹æ•°æ®æºä¸‹è½½å†å² K çº¿æ•°æ®",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python scripts/prepare_crypto_data.py --trading-pairs BTC-USDT ETH-USDT
  python scripts/prepare_crypto_data.py --interval 4h --start-date 2024-01-01

æ•°æ®æº: https://data.binance.vision/
ç¯å¢ƒå˜é‡:
  ALGVEX_DATA_DIR  - è‡ªå®šä¹‰æ•°æ®ç›®å½•
  HTTPS_PROXY      - ä»£ç†æœåŠ¡å™¨
        """
    )

    parser.add_argument(
        "--trading-pairs", type=str, nargs="+",
        default=["BTC-USDT", "ETH-USDT"],
        help="äº¤æ˜“å¯¹åˆ—è¡¨ (é»˜è®¤: BTC-USDT ETH-USDT)",
    )
    parser.add_argument(
        "--interval", type=str, default="1h",
        choices=SUPPORTED_INTERVALS,
        help="Kçº¿é—´éš” (é»˜è®¤: 1h)",
    )
    parser.add_argument(
        "--start-date", type=str, default="2023-01-01",
        help="å¼€å§‹æ—¥æœŸ YYYY-MM-DD",
    )
    parser.add_argument(
        "--end-date", type=str, default="2024-12-31",
        help="ç»“æŸæ—¥æœŸ YYYY-MM-DD",
    )
    parser.add_argument(
        "--output-dir", type=str, default=None,
        help="è¾“å‡ºç›®å½• (é»˜è®¤: ~/.algvex/data)",
    )

    # å…¼å®¹æ—§å‚æ•° (å¿½ç•¥)
    parser.add_argument("--sync", action="store_true", help=argparse.SUPPRESS)
    parser.add_argument("--proxy", type=str, help=argparse.SUPPRESS)
    parser.add_argument("--api-base", type=str, help=argparse.SUPPRESS)

    args = parser.parse_args()

    # =========================================================================
    # åˆå§‹åŒ–
    # =========================================================================
    print("=" * 60)
    print("AlgVex æ•°æ®å‡†å¤‡å·¥å…· v10.2.0")
    print("æ•°æ®æº: data.binance.vision (Binance å®˜æ–¹)")
    print("=" * 60)

    # æ£€æŸ¥ä¾èµ–
    try:
        import pyarrow
    except ImportError:
        print("\nâŒ ç¼ºå°‘ä¾èµ–: pyarrow")
        print("   pip install pyarrow")
        sys.exit(1)

    # ä»£ç†é…ç½®
    proxies = get_proxy_config()
    if proxies:
        print(f"\nğŸŒ ä»£ç†: {proxies['https']}")
    else:
        print("\nğŸŒ æœªé…ç½®ä»£ç† (å¦‚éœ€ä»£ç†ï¼Œè®¾ç½® HTTPS_PROXY ç¯å¢ƒå˜é‡)")

    # è§£ææ—¥æœŸ
    try:
        start_date = datetime.strptime(args.start_date, "%Y-%m-%d").date()
        end_date = datetime.strptime(args.end_date, "%Y-%m-%d").date()
    except ValueError as e:
        print(f"\nâŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {e}")
        sys.exit(1)

    if start_date >= end_date:
        print("\nâŒ start-date å¿…é¡»æ—©äº end-date")
        sys.exit(1)

    # è¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir).expanduser() if args.output_dir else get_default_data_dir()

    # è½¬æ¢äº¤æ˜“å¯¹
    symbols = [trading_pair_to_symbol(p) for p in args.trading_pairs]

    print(f"\nğŸ“Š é…ç½®:")
    print(f"   äº¤æ˜“å¯¹: {', '.join(symbols)}")
    print(f"   é—´éš”: {args.interval}")
    print(f"   æ—¶é—´èŒƒå›´: {start_date} ~ {end_date}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")

    # =========================================================================
    # ä¸‹è½½æ•°æ®
    # =========================================================================
    print("\n" + "=" * 60)
    print("ğŸ“¥ å¼€å§‹ä¸‹è½½")
    print("=" * 60)

    all_data = {}
    failed = []

    for symbol in symbols:
        raw_df = download_symbol_data(symbol, args.interval, start_date, end_date, proxies)

        if raw_df.empty:
            failed.append(symbol)
            print(f"   âš ï¸ {symbol}: æ— æ•°æ®")
            continue

        # è½¬æ¢æ ¼å¼
        parquet_df = convert_to_parquet_format(raw_df, start_date, end_date)

        if parquet_df.empty:
            failed.append(symbol)
            print(f"   âš ï¸ {symbol}: è½¬æ¢åæ— æ•°æ®")
            continue

        all_data[symbol] = parquet_df
        print(f"   âœ… {symbol}: {len(parquet_df):,} è¡Œ ({parquet_df.index.min().date()} ~ {parquet_df.index.max().date()})")

    # =========================================================================
    # ä¿å­˜æ•°æ®
    # =========================================================================
    if not all_data:
        print("\n" + "=" * 60)
        print("âŒ æœªä¸‹è½½åˆ°ä»»ä½•æ•°æ®")
        print("=" * 60)
        print("\nğŸ’¡ æ•…éšœæ’é™¤:")
        print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("   2. è®¾ç½®ä»£ç†: export HTTPS_PROXY=http://127.0.0.1:7890")
        print("   3. ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®: python scripts/generate_mock_data.py")
        sys.exit(1)

    print("\n" + "=" * 60)
    print("ğŸ’¾ ä¿å­˜æ•°æ®")
    print("=" * 60)

    save_to_parquet(all_data, output_dir, args.interval)

    # =========================================================================
    # å®Œæˆ
    # =========================================================================
    print("\n" + "=" * 60)
    print("âœ… æ•°æ®å‡†å¤‡å®Œæˆ!")
    print("=" * 60)
    print(f"   æˆåŠŸ: {', '.join(all_data.keys())}")
    print(f"   ä½ç½®: {output_dir / args.interval}")

    if failed:
        print(f"   å¤±è´¥: {', '.join(failed)}")


if __name__ == "__main__":
    main()
